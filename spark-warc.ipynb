{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, IntegerType\n",
    "import sparknlp\n",
    "spark = sparknlp.start() \n",
    "# sparknlp.start(gpu=True) >> for training on GPU\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from pyspark.ml import Pipeline\n",
    "from langdetect import detect\n",
    "from pyspark.sql.functions import col, lit, concat_ws\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "import requests\n",
    "from warcio import ArchiveIterator\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "import yfinance as yf\n",
    "import boto3\n",
    "import botocore\n",
    "import random\n",
    "import sys \n",
    "#PARAMETERS\n",
    "\n",
    "numcrawlsforrun = 1\n",
    "batch_size_max = sys.maxsize -1\n",
    "num_records_percrawl = 150 #number of recors to attempt to extract from each crawl\n",
    "ticker = 'SPY'\n",
    "#read in financewordlist.csv into the list\n",
    "wordlist = pd.read_csv('./sentdat/topics.csv', header=None)[0].tolist()\n",
    "wordlist.extend(yf.Ticker(ticker).info['longName'].split())\n",
    "\n",
    "\n",
    "#start spark sesssion\n",
    "spark = SparkSession.builder.appName(\"sentimentanalysis\")\\\n",
    ".config(\"spark.jars.packages\",\"com.johnsnowlabs.nlp:spark-nlp_2.12:4.2.3\")\\\n",
    ".getOrCreate()\n",
    "# .config(\"spark.driver.memory\",\"8G\")\\\n",
    "# .config(\"spark.driver.maxResultSize\", \"2G\")\\\n",
    "# .config(\"spark.jars\", \"file:///home/ubuntu/sparknlp.jar\")\\\n",
    "# .config(\"spark.driver.extraClassPath\", \"file:///home/ubuntu/sparknlp.jar\")\\\n",
    "# .config(\"spark.executor.extraClassPath\", \"file:///home/ubuntu/sparknlp.jar\")\\\n",
    "\n",
    "###GETTING WARC FILE NAMES FROM S3, GRABBING A RANDOM SAMPLE OF THEM\n",
    "s3 = boto3.resource('s3')\n",
    "my_bucket = s3.Bucket('commoncrawl')\n",
    "warcs = []\n",
    "for object in my_bucket.objects.filter(Prefix='crawl-data/CC-NEWS/'):\n",
    "    if object.key.endswith('.warc.gz'):\n",
    "        warcs.append(object.key)\n",
    "\n",
    "#choose 100 random warcs\n",
    "randomwarcs = random.sample(warcs, numcrawlsforrun)\n",
    "\n",
    "for index, warc in enumerate(randomwarcs):\n",
    "    randomwarcs[index] = 'https://data.commoncrawl.org/' + warc\n",
    "\n",
    "#function to convert time from commoncrawl format to y-m-d\n",
    "def convert_header_date(date):\n",
    "    return time.strftime('%Y-%m-%d', time.strptime(date, '%Y-%m-%dT%H:%M:%SZ'))\n",
    "\n",
    "\n",
    "#obtaining stock data from yahoo finance from 2019 to current date.\n",
    "currentdate = time.strftime(\"%Y-%m-%d\")\n",
    "stockdata = yf.download(ticker, start='2010-01-01', end=currentdate)['Adj Close']\n",
    "\n",
    "#creating scehma to store text and prices\n",
    "data = StructType([\\\n",
    "  StructField(\"text\", StringType(), True),\n",
    "    StructField(\"price\", StringType(), True),  \n",
    "]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_stock(plaintext, stklist=wordlist): \n",
    "    try:\n",
    "        for stk in stklist:\n",
    "            if plaintext.find(stk) != -1:\n",
    "                return True\n",
    "        return False\n",
    "    except:\n",
    "        raise Exception(\"issue with wordlist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warc done\n",
      "38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:>                                                        (0 + 12) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of data:  38\n",
      "done\n",
      "failures:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# creating the main rdd to store the data\n",
    "df = spark.createDataFrame(spark.sparkContext.emptyRDD(), data)\n",
    "list_of_rows_batch = []\n",
    "rows_batch_len = 0\n",
    "recordsfetched = 0\n",
    "failures = 0\n",
    "\n",
    "for warc_url in randomwarcs:\n",
    "    response = requests.get(warc_url, stream=True)\n",
    "    if response.ok!=True:\n",
    "        raise Exception(\"Error downloading WARC file\")\n",
    "    records = ArchiveIterator(response.raw, arc2warc=True)\n",
    "    #what this should do is write each record's plaintexxt to a csv file\n",
    "    for record in records:\n",
    "        if record.rec_type == 'response':\n",
    "            try: \n",
    "                html = record.content_stream().read() .decode('utf-8')\n",
    "                plaintext = BeautifulSoup(html, 'lxml').get_text()\n",
    "                plaintext = re.sub(r'\\s+', ' ', plaintext)\n",
    "                plaintext = re.sub(r'[^a-zA-Z0-9\\s]', '', plaintext).lower()\n",
    "\n",
    "                #obtains plaintext from the html\n",
    "                if detect(plaintext) == 'en' and len(plaintext) > 150 and contains_stock(plaintext) == False:  #TODO add classifier here later to ensure its a financial article\n",
    "                    date = record.rec_headers.get_header('WARC-Date')\n",
    "                    date = convert_header_date(date)\n",
    "                    # append the plaintext and price to the batch\n",
    "                    if date in stockdata.index:\n",
    "                        list_of_rows_batch.append({'text':plaintext, 'price':float(stockdata[date])})\n",
    "                        recordsfetched += 1\n",
    "                        rows_batch_len += 1\n",
    "                    else:\n",
    "                        print('date not in stockdata',date)\n",
    "                        #likely a weekend or holiday, so we will just skip the entire warc\n",
    "                        break\n",
    "                else:\n",
    "                    recordsfetched += 1                          \n",
    "            except:\n",
    "                recordsfetched += 1  # because if the entire warc file is not in english or wrong date, we still want to move on to the next one\n",
    "                failures += 1\n",
    "                #print(\"attempt record: \", record.rec_headers.get_header('WARC-Target-URI'), \" failed\")\n",
    "                pass\n",
    "\n",
    "        if rows_batch_len >= batch_size_max: \n",
    "            batchdf = spark.createDataFrame(list_of_rows_batch, data)\n",
    "            print(\"union started\")\n",
    "            df = df.union(batchdf)\n",
    "            print(\"union done\")\n",
    "            print(df.count())\n",
    "            rows_batch_len = 0\n",
    "            list_of_rows_batch = []\n",
    "        if recordsfetched >= num_records_percrawl:\n",
    "            recordsfetched = 0\n",
    "            print(\"warc done\")\n",
    "            break\n",
    "\n",
    "    #finishing up for the last batch in it wasn't full and num batches wasnt maxed out.\n",
    "if rows_batch_len > 0:\n",
    "    print(rows_batch_len)\n",
    "    batchdf = spark.createDataFrame(list_of_rows_batch, data)\n",
    "    df = df.union(batchdf)\n",
    "    print(\"size of data: \", df.count())\n",
    "    rows_batch_len = 0\n",
    "print(\"done\")\n",
    "print(\"failures: \", failures)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "ratio_com_yfin = 1\n",
    "math.floor(df.count()*ratio_com_yfin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#NOW THE DATA IS IN THE SPARK DATAFRAME. wE ARE TRAINING CLASSIFICATION MODEL ON FINANCIAL NEWS DATA and commoncrawl data\n",
    "#which has been sorted by keywords to ensure it is not financial news.\n",
    "#credit to the financial news articles at https://www.kaggle.com/datasets/jeet2016/us-financial-news-articles\n",
    "#read in all the json files into a dataframe from 2018_01_112b52537b67659ad3609a234388c50a\n",
    "\n",
    "articles = spark.read.json('./data/2018_01_112b52537b67659ad3609a234388c50a/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "articles = articles.withColumn('price', lit(0))\n",
    "articles = articles.withColumn('financial', lit(1))\n",
    "cols = articles.columns\n",
    "for item in ['text', 'price', 'financial']:\n",
    "    cols.remove(item)\n",
    "articles = articles.drop(*cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#write articles to a parquet file\n",
    "articles.write.parquet('./articlespar.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "57802"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles = spark.read.parquet('./articlespar.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df = df.withColumn('financial', lit(0))\n",
    "df = df.union(articles)\n",
    "\n",
    "#split the data into training and testing\n",
    "train, test = df.randomSplit([0.8, 0.2], seed=3204123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[ | ]lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[ / ]Download done! Loading the resource.\n",
      "[OK!]\n",
      "bert_base_cased download started this may take some time.\n",
      "Approximate size to download 389.1 MB\n",
      "[ | ]bert_base_cased download started this may take some time.\n",
      "Approximate size to download 389.1 MB\n",
      "[ / ]Download done! Loading the resource.\n",
      "[ | ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-10 21:03:31.746316: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# preprocess the text data\n",
    "document_assembler = DocumentAssembler()\\\n",
    "    .setInputCol(\"text\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "tokenizer = Tokenizer()\\\n",
    "    .setInputCols([\"document\"])\\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "normalizer = Normalizer()\\\n",
    "    .setInputCols([\"token\"])\\\n",
    "    .setOutputCol(\"normalized\")\\\n",
    "    .setLowercase(True)\n",
    "\n",
    "stopwords_cleaner = StopWordsCleaner()\\\n",
    "    .setInputCols(\"normalized\")\\\n",
    "    .setOutputCol(\"cleanTokens\")\\\n",
    "    .setCaseSensitive(False)\n",
    "\n",
    "lemma = LemmatizerModel.pretrained(\"lemma_antbnc\")\\\n",
    "    .setInputCols([\"cleanTokens\"])\\\n",
    "    .setOutputCol(\"lemma\")\n",
    "\n",
    "word_embeddings = BertEmbeddings\\\n",
    "    .pretrained('bert_base_cased', 'en') \\\n",
    "    .setInputCols([\"document\",'lemma'])\\\n",
    "    .setOutputCol(\"embeddings\")\\\n",
    "\n",
    "# word_embeddings = AlbertEmbeddings.pretrained('albert_base_uncased', 'en') \\ #lighter eight bert embeddings\n",
    "#     .setInputCols([\"document\",'lemma'])\\\n",
    "#     .setOutputCol(\"embeddings\")\\\n",
    "\n",
    "# https://nlp.johnsnowlabs.com/docs/en/transformers#bertsentenceembeddings? better for sentence embeddings for later models, this one words is better\n",
    "#https://nlp.johnsnowlabs.com/docs/en/transformers#debertaembeddings\n",
    "#lots of transfoemrs to choose from for later tasks, for this one lightweight bert might be the best\n",
    "embeddingsSentence = SentenceEmbeddings()\\\n",
    "    .setInputCols([\"document\", \"embeddings\"])\\\n",
    "    .setOutputCol(\"sentence_embeddings\")\\\n",
    "    .setPoolingStrategy(\"AVERAGE\")\n",
    "\n",
    "classifierdl = ClassifierDLApproach()\\\n",
    "    .setInputCols([\"sentence_embeddings\"])\\\n",
    "    .setOutputCol(\"financial_model_pred\")\\\n",
    "    .setLabelColumn(\"financial\")\\\n",
    "    .setMaxEpochs(5)\\\n",
    "        .setEnableOutputLogs(True)\\\n",
    "    .setLr(0.001)\\\n",
    "\n",
    "DLpipeline = Pipeline(\n",
    "    stages = [\n",
    "        document_assembler,\n",
    "        tokenizer,\n",
    "        normalizer,\n",
    "        stopwords_cleaner,\n",
    "        lemma,\n",
    "        word_embeddings,\n",
    "        embeddingsSentence,\n",
    "        classifierdl\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-10 21:05:40.291373: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /tmp/a0d89cc9ddf7_classifier_dl7804348254998463337\n",
      "2022-12-10 21:05:40.690361: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:107] Reading meta graph with tags { serve }\n",
      "2022-12-10 21:05:40.690901: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:148] Reading SavedModel debug info (if present) from: /tmp/a0d89cc9ddf7_classifier_dl7804348254998463337\n",
      "2022-12-10 21:05:42.023701: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2022-12-10 21:05:43.255010: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: /tmp/a0d89cc9ddf7_classifier_dl7804348254998463337\n",
      "2022-12-10 21:05:43.503337: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 3212453 microseconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started - epochs: 5 - learning_rate: 0.001 - batch_size: 64 - training_examples: 112 - classes: 2\n",
      "Epoch 1/5 - 0.44s - loss: 1.7652658 - acc: 1.0 - batches: 2\n",
      "Epoch 2/5 - 0.02s - loss: 1.3402622 - acc: 1.0 - batches: 2\n",
      "Epoch 3/5 - 0.02s - loss: 1.0547364 - acc: 1.0 - batches: 2\n",
      "Epoch 4/5 - 0.02s - loss: 0.87432295 - acc: 1.0 - batches: 2\n",
      "Epoch 5/5 - 0.02s - loss: 0.80400145 - acc: 1.0 - batches: 2\n",
      "done training\n",
      "done predicting, here are results on the test set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 46:===================================================>    (12 + 1) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00        16\n",
      "\n",
      "    accuracy                           1.00        26\n",
      "   macro avg       1.00      1.00      1.00        26\n",
      "weighted avg       1.00      1.00      1.00        26\n",
      " green\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "DLpipelineModel = DLpipeline.fit(train)\n",
    "DLpipelineModel.save(\"dl_model\")\n",
    "# model = PipelineModel.load(\"dl_model\")\n",
    "print(\"done training\")\n",
    "test_predict = DLpipelineModel.transform(test)\n",
    "\n",
    "\n",
    "\n",
    "results = test_predict.select('text','price', 'financial','financial_model_pred.result')\n",
    "results = results.withColumn('result', results['result'].getItem(0).cast('float'))\n",
    "\n",
    "results = results.withColumn('result', results['result'].cast('float'))\n",
    "print(\"done predicting, here are results on the test set\")\n",
    "print(classification_report(results.select('financial').collect(), results.select('result').collect()), 'green')\n",
    "\n",
    "#https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/transformers/HuggingFace%20in%20Spark%20NLP%20-%20BERT.ipynb\n",
    "#https://towardsdatascience.com/text-classification-in-spark-nlp-with-bert-and-universal-sentence-encoders-e644d618ca32\n",
    "#can get bert from there, then create a piepline that uses the bert model to get embeddings, then use the embeddings to train a classifier\n",
    "#then we conver this to a script, upload to emr and get a large scale model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NEED TO PROCESS THE BAG OF BIG WORDS, dont need to run this every time\n",
    "#http://mpqa.cs.pitt.edu/lexicons/subj_lexicon/ from this source for academic sentiment analysis\n",
    "#copy sentiment-big.tff to a txt file\n",
    "#then use this to create a csv file\n",
    "\n",
    "sentimentprocess = pd.read_csv('./sentdat/sentiment-big.txt', sep=' ', header=None)\n",
    "#remove all columns except 5 and 0\n",
    "sentimentprocess = sentimentprocess[[2,5]]\n",
    "\n",
    "#strip type= from column 0\n",
    "sentimentprocess[2] = sentimentprocess[2].str.replace('word1=', '')\n",
    "#strip priorpolarity= from column 5\n",
    "sentimentprocess[5] = sentimentprocess[5].str.replace('priorpolarity=', '')\n",
    "sentimentprocess.head(10)\n",
    "#save to csv\n",
    "sentimentprocess.to_csv('./sentdat/sentiment-big.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark NLP version 4.2.3\n",
      "Apache Spark version: 3.3.1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"Spark NLP version\", sparknlp.version())\n",
    "print(\"Apache Spark version:\", spark.version)\n",
    "\n",
    "#model = NerDLModel.pretrained('ner_dl')\n",
    "\n",
    "# doc_df=documentAssembler.transform(df)\n",
    "# doc_df.show()\n",
    "#has been transformed into a text, vector column\n",
    "#https://nlp.johnsnowlabs.com/2022/09/06/finclf_bert_sentiment_en.html\n",
    "\n",
    "# sort data by language? use different models for different languages?\n",
    "#train own model for english with amazon reviews?\n",
    "\n",
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol('text') \\\n",
    "    .setOutputCol('document')\n",
    "\n",
    "sentence_detector = SentenceDetector() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"sentence\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols(['sentence']) \\\n",
    "    .setOutputCol('token')\n",
    "\n",
    "# normalizer = Normalizer() \\\n",
    "#     .setInputCols(['token']) \\\n",
    "#     .setOutputCol('normalized') \\\n",
    "\n",
    "lemmatizer = Lemmatizer()\\\n",
    "    .setInputCols(['token'])\\\n",
    "    .setOutputCol('lemma')\\\n",
    "  .setDictionary(\"./sentdat/lemmas_small.txt\", key_delimiter=\"->\", value_delimiter=\"\\t\")\n",
    "#! wget -N https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/lemma-corpus-small/lemmas_small.txt -P /tmp\n",
    "SentimentDetector = sentiment.SentimentDetector() \\\n",
    "    .setInputCols(['lemma', 'sentence'])\\\n",
    "    .setOutputCol('sentiment_score')\\\n",
    "    .setDictionary('./sentdat/sentiment-big.csv', ',')\\\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    document_assembler, \n",
    "    sentence_detector,\n",
    "    tokenizer,\n",
    "    lemmatizer,\n",
    "    SentimentDetector\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget -N https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/sentiment-corpus/default-sentiment-dict.txt -P ./sentdat \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget -N https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/lemma-corpus-small/lemmas_small.txt -P ./sentdat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf = pipeline.fit(df).transform(df)\n",
    "#getting a new df with sentiment score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+--------------------+--------------------+--------------------+--------------------+---------------+\n",
      "|                text|             price|            document|            sentence|               token|               lemma|sentiment_score|\n",
      "+--------------------+------------------+--------------------+--------------------+--------------------+--------------------+---------------+\n",
      "| Chicago Med Seas...|394.69000244140625|[{document, 0, 14...|[{document, 1, 64...|[{token, 1, 7, Ch...|[{token, 1, 7, Ch...|       positive|\n",
      "| Roberto Guilherm...|394.69000244140625|[{document, 0, 60...|[{document, 1, 13...|[{token, 1, 7, Ro...|[{token, 1, 7, Ro...|       positive|\n",
      "+--------------------+------------------+--------------------+--------------------+--------------------+--------------------+---------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "newdf = newdf.withColumn(\"sentiment_score\", concat_ws(\",\", \"sentiment_score.result\"))\n",
    "newdf.show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positives 77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 61:==============================================>         (10 + 2) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negatives 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"positives\", newdf.filter(col('sentiment_score') == 'positive').count())\n",
    "print(\"negatives\", newdf.filter(col('sentiment_score') == 'negative').count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('am111_venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f57737e6d1a902c43fc81f11403a4faf60173bd712443272aa63091abb3c628b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
