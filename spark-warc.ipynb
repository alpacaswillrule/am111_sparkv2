{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/13 16:56:08 WARN Utils: Your hostname, DESKTOP-QC3P089 resolves to a loopback address: 127.0.1.1; using 172.23.92.21 instead (on interface eth0)\n",
      "22/12/13 16:56:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/home/johan/am111/am111_venv/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/johan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/johan/.ivy2/jars\n",
      "com.johnsnowlabs.nlp#spark-nlp_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-b5a03c38-b642-404a-b1b6-def2d97e5d0d;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.johnsnowlabs.nlp#spark-nlp_2.12;4.2.4 in central\n",
      "\tfound com.typesafe#config;1.4.2 in central\n",
      "\tfound org.rocksdb#rocksdbjni;6.29.5 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.828 in central\n",
      "\tfound com.github.universal-automata#liblevenshtein;3.0.0 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.0.0-beta-3 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.0.0-beta-3 in central\n",
      "\tfound com.google.code.gson#gson;2.3 in central\n",
      "\tfound it.unimi.dsi#fastutil;7.0.12 in central\n",
      "\tfound org.projectlombok#lombok;1.16.8 in central\n",
      "\tfound com.google.cloud#google-cloud-storage;2.15.0 in central\n",
      "\tfound com.google.guava#guava;31.1-jre in central\n",
      "\tfound com.google.guava#failureaccess;1.0.1 in central\n",
      "\tfound com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava in central\n",
      "\tfound com.google.j2objc#j2objc-annotations;1.3 in central\n",
      "\tfound com.google.http-client#google-http-client;1.42.3 in central\n",
      "\tfound io.opencensus#opencensus-contrib-http-util;0.31.1 in central\n",
      "\tfound com.google.http-client#google-http-client-jackson2;1.42.3 in central\n",
      "\tfound com.google.http-client#google-http-client-gson;1.42.3 in central\n",
      "\tfound com.google.api-client#google-api-client;2.0.1 in central\n",
      "\tfound com.google.oauth-client#google-oauth-client;1.34.1 in central\n",
      "\tfound com.google.http-client#google-http-client-apache-v2;1.42.3 in central\n",
      "\tfound com.google.apis#google-api-services-storage;v1-rev20220705-2.0.0 in central\n",
      "\tfound com.google.code.gson#gson;2.10 in central\n",
      "\tfound com.google.cloud#google-cloud-core;2.8.27 in central\n",
      "\tfound com.google.auto.value#auto-value-annotations;1.10 in central\n",
      "\tfound com.google.cloud#google-cloud-core-http;2.8.27 in central\n",
      "\tfound com.google.http-client#google-http-client-appengine;1.42.3 in central\n",
      "\tfound com.google.api#gax-httpjson;0.104.5 in central\n",
      "\tfound com.google.cloud#google-cloud-core-grpc;2.8.27 in central\n",
      "\tfound io.grpc#grpc-core;1.50.2 in central\n",
      "\tfound com.google.api#gax;2.19.5 in central\n",
      "\tfound com.google.api#gax-grpc;2.19.5 in central\n",
      "\tfound com.google.auth#google-auth-library-credentials;1.12.1 in central\n",
      "\tfound com.google.auth#google-auth-library-oauth2-http;1.12.1 in central\n",
      "\tfound com.google.api#api-common;2.2.2 in central\n",
      "\tfound javax.annotation#javax.annotation-api;1.3.2 in central\n",
      "\tfound io.opencensus#opencensus-api;0.31.1 in central\n",
      "\tfound io.grpc#grpc-context;1.50.2 in central\n",
      "\tfound com.google.api.grpc#proto-google-iam-v1;1.6.7 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.21.9 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.21.9 in central\n",
      "\tfound com.google.api.grpc#proto-google-common-protos;2.10.0 in central\n",
      "\tfound org.threeten#threetenbp;1.6.4 in central\n",
      "\tfound com.google.api.grpc#proto-google-cloud-storage-v2;2.15.0-alpha in central\n",
      "\tfound com.google.api.grpc#grpc-google-cloud-storage-v2;2.15.0-alpha in central\n",
      "\tfound io.grpc#grpc-protobuf;1.50.2 in central\n",
      "\tfound com.google.api.grpc#gapic-google-cloud-storage-v2;2.15.0-alpha in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.14.0 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound io.grpc#grpc-api;1.50.2 in central\n",
      "\tfound io.grpc#grpc-auth;1.50.2 in central\n",
      "\tfound io.grpc#grpc-stub;1.50.2 in central\n",
      "\tfound org.checkerframework#checker-qual;3.27.0 in central\n",
      "\tfound com.google.api.grpc#grpc-google-iam-v1;1.6.7 in central\n",
      "\tfound io.grpc#grpc-protobuf-lite;1.50.2 in central\n",
      "\tfound com.google.android#annotations;4.1.1.4 in central\n",
      "\tfound org.codehaus.mojo#animal-sniffer-annotations;1.22 in central\n",
      "\tfound io.grpc#grpc-alts;1.50.2 in central\n",
      "\tfound io.grpc#grpc-grpclb;1.50.2 in central\n",
      "\tfound org.conscrypt#conscrypt-openjdk-uber;2.5.2 in central\n",
      "\tfound io.grpc#grpc-netty-shaded;1.50.2 in central\n",
      "\tfound io.perfmark#perfmark-api;0.25.0 in central\n",
      "\tfound io.grpc#grpc-googleapis;1.50.2 in central\n",
      "\tfound io.grpc#grpc-xds;1.50.2 in central\n",
      "\tfound io.opencensus#opencensus-proto;0.2.0 in central\n",
      "\tfound io.grpc#grpc-services;1.50.2 in central\n",
      "\tfound com.google.re2j#re2j;1.6 in central\n",
      "\tfound com.navigamez#greex;1.0 in central\n",
      "\tfound dk.brics.automaton#automaton;1.11-8 in central\n",
      "\tfound com.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.4 in central\n",
      ":: resolution report :: resolve 1180ms :: artifacts dl 60ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.828 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.14.0 from central in [default]\n",
      "\tcom.github.universal-automata#liblevenshtein;3.0.0 from central in [default]\n",
      "\tcom.google.android#annotations;4.1.1.4 from central in [default]\n",
      "\tcom.google.api#api-common;2.2.2 from central in [default]\n",
      "\tcom.google.api#gax;2.19.5 from central in [default]\n",
      "\tcom.google.api#gax-grpc;2.19.5 from central in [default]\n",
      "\tcom.google.api#gax-httpjson;0.104.5 from central in [default]\n",
      "\tcom.google.api-client#google-api-client;2.0.1 from central in [default]\n",
      "\tcom.google.api.grpc#gapic-google-cloud-storage-v2;2.15.0-alpha from central in [default]\n",
      "\tcom.google.api.grpc#grpc-google-cloud-storage-v2;2.15.0-alpha from central in [default]\n",
      "\tcom.google.api.grpc#grpc-google-iam-v1;1.6.7 from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-cloud-storage-v2;2.15.0-alpha from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-common-protos;2.10.0 from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-iam-v1;1.6.7 from central in [default]\n",
      "\tcom.google.apis#google-api-services-storage;v1-rev20220705-2.0.0 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-credentials;1.12.1 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-oauth2-http;1.12.1 from central in [default]\n",
      "\tcom.google.auto.value#auto-value-annotations;1.10 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core;2.8.27 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core-grpc;2.8.27 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core-http;2.8.27 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-storage;2.15.0 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.10 from central in [default]\n",
      "\tcom.google.guava#failureaccess;1.0.1 from central in [default]\n",
      "\tcom.google.guava#guava;31.1-jre from central in [default]\n",
      "\tcom.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava from central in [default]\n",
      "\tcom.google.http-client#google-http-client;1.42.3 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-apache-v2;1.42.3 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-appengine;1.42.3 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-gson;1.42.3 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-jackson2;1.42.3 from central in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;1.3 from central in [default]\n",
      "\tcom.google.oauth-client#google-oauth-client;1.34.1 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.21.9 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.21.9 from central in [default]\n",
      "\tcom.google.re2j#re2j;1.6 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#spark-nlp_2.12;4.2.4 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.4 from central in [default]\n",
      "\tcom.navigamez#greex;1.0 from central in [default]\n",
      "\tcom.typesafe#config;1.4.2 from central in [default]\n",
      "\tdk.brics.automaton#automaton;1.11-8 from central in [default]\n",
      "\tio.grpc#grpc-alts;1.50.2 from central in [default]\n",
      "\tio.grpc#grpc-api;1.50.2 from central in [default]\n",
      "\tio.grpc#grpc-auth;1.50.2 from central in [default]\n",
      "\tio.grpc#grpc-context;1.50.2 from central in [default]\n",
      "\tio.grpc#grpc-core;1.50.2 from central in [default]\n",
      "\tio.grpc#grpc-googleapis;1.50.2 from central in [default]\n",
      "\tio.grpc#grpc-grpclb;1.50.2 from central in [default]\n",
      "\tio.grpc#grpc-netty-shaded;1.50.2 from central in [default]\n",
      "\tio.grpc#grpc-protobuf;1.50.2 from central in [default]\n",
      "\tio.grpc#grpc-protobuf-lite;1.50.2 from central in [default]\n",
      "\tio.grpc#grpc-services;1.50.2 from central in [default]\n",
      "\tio.grpc#grpc-stub;1.50.2 from central in [default]\n",
      "\tio.grpc#grpc-xds;1.50.2 from central in [default]\n",
      "\tio.opencensus#opencensus-api;0.31.1 from central in [default]\n",
      "\tio.opencensus#opencensus-contrib-http-util;0.31.1 from central in [default]\n",
      "\tio.opencensus#opencensus-proto;0.2.0 from central in [default]\n",
      "\tio.perfmark#perfmark-api;0.25.0 from central in [default]\n",
      "\tit.unimi.dsi#fastutil;7.0.12 from central in [default]\n",
      "\tjavax.annotation#javax.annotation-api;1.3.2 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.27.0 from central in [default]\n",
      "\torg.codehaus.mojo#animal-sniffer-annotations;1.22 from central in [default]\n",
      "\torg.conscrypt#conscrypt-openjdk-uber;2.5.2 from central in [default]\n",
      "\torg.projectlombok#lombok;1.16.8 from central in [default]\n",
      "\torg.rocksdb#rocksdbjni;6.29.5 from central in [default]\n",
      "\torg.threeten#threetenbp;1.6.4 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcom.google.protobuf#protobuf-java-util;3.0.0-beta-3 by [com.google.protobuf#protobuf-java-util;3.21.9] in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.0.0-beta-3 by [com.google.protobuf#protobuf-java;3.21.9] in [default]\n",
      "\tcom.google.code.gson#gson;2.3 by [com.google.code.gson#gson;2.10] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   71  |   0   |   0   |   3   ||   68  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-b5a03c38-b642-404a-b1b6-def2d97e5d0d\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 68 already retrieved (0kB/36ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/13 16:56:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/13 16:56:20 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, IntegerType\n",
    "import sparknlp\n",
    "spark = sparknlp.start() \n",
    "# sparknlp.start(gpu=True) >> for training on GPU\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from pyspark.ml import Pipeline\n",
    "from langdetect import detect\n",
    "from pyspark.sql.functions import col, lit, concat_ws\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "import requests\n",
    "from warcio import ArchiveIterator\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "import yfinance as yf\n",
    "import boto3\n",
    "import botocore\n",
    "import random\n",
    "import sys \n",
    "#PARAMETERS\n",
    "\n",
    "numcrawlsforrun = 1\n",
    "batch_size_max = sys.maxsize -1\n",
    "num_records_percrawl = 100 #number of recors to attempt to extract from each crawl\n",
    "ticker = 'SPY'\n",
    "#read in financewordlist.csv into the list\n",
    "wordlist = pd.read_csv('./sentdat/topics.csv', header=None)[0].tolist()\n",
    "wordlist.extend(yf.Ticker(ticker).info['longName'].split())\n",
    "\n",
    "\n",
    "#start spark sesssion\n",
    "spark = SparkSession.builder.appName(\"sentimentanalysis\")\\\n",
    ".config(\"spark.jars.packages\",\"com.johnsnowlabs.nlp:spark-nlp_2.12:4.2.3\")\\\n",
    ".getOrCreate()\n",
    "# .config(\"spark.driver.memory\",\"8G\")\\\n",
    "# .config(\"spark.driver.maxResultSize\", \"2G\")\\\n",
    "# .config(\"spark.jars\", \"file:///home/ubuntu/sparknlp.jar\")\\\n",
    "# .config(\"spark.driver.extraClassPath\", \"file:///home/ubuntu/sparknlp.jar\")\\\n",
    "# .config(\"spark.executor.extraClassPath\", \"file:///home/ubuntu/sparknlp.jar\")\\\n",
    "\n",
    "###GETTING WARC FILE NAMES FROM S3, GRABBING A RANDOM SAMPLE OF THEM\n",
    "s3 = boto3.resource('s3')\n",
    "my_bucket = s3.Bucket('commoncrawl')\n",
    "warcs = []\n",
    "for object in my_bucket.objects.filter(Prefix='crawl-data/CC-NEWS/'):\n",
    "    if object.key.endswith('.warc.gz'):\n",
    "        warcs.append(object.key)\n",
    "\n",
    "#choose 100 random warcs\n",
    "randomwarcs = random.sample(warcs, numcrawlsforrun)\n",
    "\n",
    "for index, warc in enumerate(randomwarcs):\n",
    "    randomwarcs[index] = 'https://data.commoncrawl.org/' + warc\n",
    "\n",
    "#function to convert time from commoncrawl format to y-m-d\n",
    "def convert_header_date(date):\n",
    "    return time.strftime('%Y-%m-%d', time.strptime(date, '%Y-%m-%dT%H:%M:%SZ'))\n",
    "\n",
    "\n",
    "#obtaining stock data from yahoo finance from 2019 to current date.\n",
    "currentdate = time.strftime(\"%Y-%m-%d\")\n",
    "stockdata = yf.download(ticker, start='2010-01-01', end=currentdate)['Adj Close']\n",
    "\n",
    "#creating scehma to store text and prices\n",
    "data = StructType([\\\n",
    "  StructField(\"text\", StringType(), True),\n",
    "    StructField(\"price\", StringType(), True),  \n",
    "]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_stock(plaintext, stklist=wordlist): \n",
    "    try:\n",
    "        for stk in stklist:\n",
    "            if plaintext.find(stk) != -1:\n",
    "                return True\n",
    "        return False\n",
    "    except:\n",
    "        raise Exception(\"issue with wordlist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warc done\n",
      "65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                        (0 + 12) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of data:  65\n",
      "done\n",
      "failures:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# creating the main rdd to store the data\n",
    "df = spark.createDataFrame(spark.sparkContext.emptyRDD(), data)\n",
    "list_of_rows_batch = []\n",
    "rows_batch_len = 0\n",
    "recordsfetched = 0\n",
    "failures = 0\n",
    "\n",
    "for warc_url in randomwarcs:\n",
    "    response = requests.get(warc_url, stream=True)\n",
    "    if response.ok!=True:\n",
    "        raise Exception(\"Error downloading WARC file\")\n",
    "    records = ArchiveIterator(response.raw, arc2warc=True)\n",
    "    #what this should do is write each record's plaintexxt to a csv file\n",
    "    for record in records:\n",
    "        if record.rec_type == 'response':\n",
    "            try: \n",
    "                html = record.content_stream().read() .decode('utf-8')\n",
    "                plaintext = BeautifulSoup(html, 'lxml').get_text()\n",
    "                plaintext = re.sub(r'\\s+', ' ', plaintext)\n",
    "                plaintext = re.sub(r'[^a-zA-Z0-9\\s]', '', plaintext).lower()\n",
    "\n",
    "                #obtains plaintext from the html\n",
    "                if detect(plaintext) == 'en' and len(plaintext) > 150:  #TODO add classifier here later to ensure its a financial article\n",
    "                    date = record.rec_headers.get_header('WARC-Date')\n",
    "                    date = convert_header_date(date)\n",
    "                    # append the plaintext and price to the batch\n",
    "                    if date in stockdata.index:\n",
    "                        list_of_rows_batch.append({'text':plaintext, 'price':float(stockdata[date])})\n",
    "                        recordsfetched += 1\n",
    "                        rows_batch_len += 1\n",
    "                    else:\n",
    "                        print('date not in stockdata',date)\n",
    "                        #likely a weekend or holiday, so we will just skip the entire warc\n",
    "                        break\n",
    "                else:\n",
    "                    recordsfetched += 1                          \n",
    "            except:\n",
    "                recordsfetched += 1  # because if the entire warc file is not in english or wrong date, we still want to move on to the next one\n",
    "                failures += 1\n",
    "                #print(\"attempt record: \", record.rec_headers.get_header('WARC-Target-URI'), \" failed\")\n",
    "                pass\n",
    "\n",
    "        if rows_batch_len >= batch_size_max: \n",
    "            batchdf = spark.createDataFrame(list_of_rows_batch, data)\n",
    "            print(\"union started\")\n",
    "            df = df.union(batchdf)\n",
    "            print(\"union done\")\n",
    "            print(df.count())\n",
    "            rows_batch_len = 0\n",
    "            list_of_rows_batch = []\n",
    "        if recordsfetched >= num_records_percrawl:\n",
    "            recordsfetched = 0\n",
    "            print(\"warc done\")\n",
    "            break\n",
    "\n",
    "    #finishing up for the last batch in it wasn't full and num batches wasnt maxed out.\n",
    "if rows_batch_len > 0:\n",
    "    print(rows_batch_len)\n",
    "    batchdf = spark.createDataFrame(list_of_rows_batch, data)\n",
    "    df = df.union(batchdf)\n",
    "    print(\"size of data: \", df.count())\n",
    "    rows_batch_len = 0\n",
    "print(\"done\")\n",
    "print(\"failures: \", failures)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "ratio_com_yfin = 1\n",
    "math.floor(df.count()*ratio_com_yfin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOW THE DATA IS IN THE SPARK DATAFRAME. wE ARE TRAINING CLASSIFICATION MODEL ON FINANCIAL NEWS DATA and commoncrawl data\n",
    "#which has been sorted by keywords to ensure it is not financial news.\n",
    "#credit to the financial news articles at https://www.kaggle.com/datasets/jeet2016/us-financial-news-articles\n",
    "#read in all the json files into a dataframe from 2018_01_112b52537b67659ad3609a234388c50a\n",
    "\n",
    "articles = spark.read.json('./data/2018_01_112b52537b67659ad3609a234388c50a/').limit(df.count()*ratio_com_yfin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "articles = articles.withColumn('price', lit(0))\n",
    "articles = articles.withColumn('financial', lit(1))\n",
    "cols = articles.columns\n",
    "for item in ['text', 'price', 'financial']:\n",
    "    cols.remove(item)\n",
    "articles = articles.drop(*cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#write articles to a parquet file\n",
    "articles.write.parquet('./articlespar.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "articles = spark.read.parquet('./articlespar.parquet').limit(df.count()*ratio_com_yfin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df = df.withColumn('financial', lit(0))\n",
    "df = df.union(articles)\n",
    "\n",
    "#split the data into training and testing\n",
    "train, test = df.randomSplit([0.8, 0.2], seed=3204123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[ | ]lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "Download done! Loading the resource.\n",
      "[OK!]\n",
      "bert_base_cased download started this may take some time.\n",
      "Approximate size to download 389.1 MB\n",
      "[ | ]bert_base_cased download started this may take some time.\n",
      "Approximate size to download 389.1 MB\n",
      "Download done! Loading the resource.\n",
      "[ â€” ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-13 16:58:18.771184: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# preprocess the text data\n",
    "document_assembler = DocumentAssembler()\\\n",
    "    .setInputCol(\"text\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "tokenizer = Tokenizer()\\\n",
    "    .setInputCols([\"document\"])\\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "normalizer = Normalizer()\\\n",
    "    .setInputCols([\"token\"])\\\n",
    "    .setOutputCol(\"normalized\")\\\n",
    "    .setLowercase(True)\n",
    "\n",
    "stopwords_cleaner = StopWordsCleaner()\\\n",
    "    .setInputCols(\"normalized\")\\\n",
    "    .setOutputCol(\"cleanTokens\")\\\n",
    "    .setCaseSensitive(False)\n",
    "\n",
    "lemma = LemmatizerModel.pretrained(\"lemma_antbnc\")\\\n",
    "    .setInputCols([\"cleanTokens\"])\\\n",
    "    .setOutputCol(\"lemma\")\n",
    "\n",
    "word_embeddings = BertEmbeddings\\\n",
    "    .pretrained('bert_base_cased', 'en') \\\n",
    "    .setInputCols([\"document\",'lemma'])\\\n",
    "    .setOutputCol(\"embeddings\")\\\n",
    "\n",
    "# word_embeddings = AlbertEmbeddings.pretrained('albert_base_uncased', 'en') \\ #lighter eight bert embeddings\n",
    "#     .setInputCols([\"document\",'lemma'])\\\n",
    "#     .setOutputCol(\"embeddings\")\\\n",
    "\n",
    "# https://nlp.johnsnowlabs.com/docs/en/transformers#bertsentenceembeddings? better for sentence embeddings for later models, this one words is better\n",
    "#https://nlp.johnsnowlabs.com/docs/en/transformers#debertaembeddings\n",
    "#lots of transfoemrs to choose from for later tasks, for this one lightweight bert might be the best\n",
    "embeddingsSentence = SentenceEmbeddings()\\\n",
    "    .setInputCols([\"document\", \"embeddings\"])\\\n",
    "    .setOutputCol(\"sentence_embeddings\")\\\n",
    "    .setPoolingStrategy(\"AVERAGE\")\n",
    "\n",
    "classifierdl = ClassifierDLApproach()\\\n",
    "    .setInputCols([\"sentence_embeddings\"])\\\n",
    "    .setOutputCol(\"financial_model_pred\")\\\n",
    "    .setLabelColumn(\"financial\")\\\n",
    "    .setMaxEpochs(5)\\\n",
    "        .setEnableOutputLogs(True)\\\n",
    "    .setLr(0.001)\\\n",
    "\n",
    "DLpipeline = Pipeline(\n",
    "    stages = [\n",
    "        document_assembler,\n",
    "        tokenizer,\n",
    "        normalizer,\n",
    "        stopwords_cleaner,\n",
    "        lemma,\n",
    "        word_embeddings,\n",
    "        embeddingsSentence,\n",
    "        classifierdl\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:===============================================>        (11 + 2) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/13 17:07:04 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 149740 ms exceeds timeout 120000 ms\n",
      "22/12/13 17:07:04 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-13 17:07:37.579647: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /tmp/228e2899eeed_classifier_dl5646696838384616029\n",
      "2022-12-13 17:07:37.643124: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:107] Reading meta graph with tags { serve }\n",
      "2022-12-13 17:07:37.643189: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:148] Reading SavedModel debug info (if present) from: /tmp/228e2899eeed_classifier_dl5646696838384616029\n",
      "2022-12-13 17:07:38.078349: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2022-12-13 17:07:39.619684: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: /tmp/228e2899eeed_classifier_dl5646696838384616029\n",
      "2022-12-13 17:07:39.780919: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 2201279 microseconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started - epochs: 5 - learning_rate: 0.001 - batch_size: 64 - training_examples: 103 - classes: 2\n",
      "Epoch 1/5 - 0.43s - loss: 1.7357066 - acc: 0.96594554 - batches: 2\n",
      "Epoch 2/5 - 0.02s - loss: 1.2325315 - acc: 1.0 - batches: 2\n",
      "Epoch 3/5 - 0.02s - loss: 1.2279463 - acc: 1.0 - batches: 2\n",
      "Epoch 4/5 - 0.02s - loss: 1.1629893 - acc: 1.0 - batches: 2\n",
      "Epoch 5/5 - 0.02s - loss: 1.086009 - acc: 1.0 - batches: 2\n",
      "done training\n"
     ]
    }
   ],
   "source": [
    "DLpipelineModel = DLpipeline.fit(train)\n",
    "DLpipelineModel.save(\"dl_model\")\n",
    "# model = PipelineModel.load(\"dl_model\")\n",
    "print(\"done training\")\n",
    "test_predict = DLpipelineModel.transform(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = test_predict.select('text','price', 'financial','financial_model_pred.result')\n",
    "results = results.withColumn('result', results['result'].getItem(0).cast('float'))\n",
    "\n",
    "# # results = results.withColumn('result', results['result'].cast('float'))\n",
    "# print(\"done predicting, here are results on the test set\")\n",
    "print(classification_report(results.select('financial').collect(), results.select('result').collect()), 'green')\n",
    "\n",
    "#https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/transformers/HuggingFace%20in%20Spark%20NLP%20-%20BERT.ipynb\n",
    "#https://towardsdatascience.com/text-classification-in-spark-nlp-with-bert-and-universal-sentence-encoders-e644d618ca32\n",
    "#can get bert from there, then create a piepline that uses the bert model to get embeddings, then use the embeddings to train a classifier\n",
    "#then we conver this to a script, upload to emr and get a large scale model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 66:==================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+---------+------+\n",
      "|                text|            price|financial|result|\n",
      "+--------------------+-----------------+---------+------+\n",
      "| boston issues ev...|443.6439514160156|        0|   [0]|\n",
      "| wildlife groups ...|443.6439514160156|        0|   [0]|\n",
      "|         dhaka pr...|443.6439514160156|        0|   [0]|\n",
      "| 8 takeaways jets...|443.6439514160156|        0|   [0]|\n",
      "| colchester firm ...|443.6439514160156|        0|   [0]|\n",
      "| cover scrapped b...|443.6439514160156|        0|   [0]|\n",
      "| hydraulic equipm...|443.6439514160156|        0|   [0]|\n",
      "|so long see you a...|443.6439514160156|        0|   [0]|\n",
      "| schools in delhi...|443.6439514160156|        0|   [0]|\n",
      "| one of our brigh...|443.6439514160156|        0|   [0]|\n",
      "| ottawa pins hope...|443.6439514160156|        0|   [0]|\n",
      "| global carbon di...|443.6439514160156|        0|   [0]|\n",
      "| mariachi by bric...|443.6439514160156|        0|   [0]|\n",
      "|taliban uses nigh...|443.6439514160156|        0|   [0]|\n",
      "| alaska lawmakers...|443.6439514160156|        0|   [0]|\n",
      "| jumper sam grewe...|443.6439514160156|        0|   [0]|\n",
      "| revisioning real...|443.6439514160156|        0|   [0]|\n",
      "|Full-Year 2017 Ne...|                0|        1|   [1]|\n",
      "|Highlights\\nQuart...|                0|        1|   [1]|\n",
      "|Key Performance H...|                0|        1|   [1]|\n",
      "|Loss per diluted ...|                0|        1|   [1]|\n",
      "|Luxembourg, Janua...|                0|        1|   [1]|\n",
      "|NEW YORK, Jan. 31...|                0|        1|   [1]|\n",
      "|OLYMPIA, Wash., J...|                0|        1|   [1]|\n",
      "|PLANO, Texas, Jan...|                0|        1|   [1]|\n",
      "|RICHMOND, Va., Ja...|                0|        1|   [1]|\n",
      "|SAN JOSE, Calif.,...|                0|        1|   [1]|\n",
      "+--------------------+-----------------+---------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "results.show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NEED TO PROCESS THE BAG OF BIG WORDS, dont need to run this every time\n",
    "#http://mpqa.cs.pitt.edu/lexicons/subj_lexicon/ from this source for academic sentiment analysis\n",
    "#copy sentiment-big.tff to a txt file\n",
    "#then use this to create a csv file\n",
    "\n",
    "sentimentprocess = pd.read_csv('./sentdat/sentiment-big.txt', sep=' ', header=None)\n",
    "#remove all columns except 5 and 0\n",
    "sentimentprocess = sentimentprocess[[2,5]]\n",
    "\n",
    "#strip type= from column 0\n",
    "sentimentprocess[2] = sentimentprocess[2].str.replace('word1=', '')\n",
    "#strip priorpolarity= from column 5\n",
    "sentimentprocess[5] = sentimentprocess[5].str.replace('priorpolarity=', '')\n",
    "sentimentprocess.head(10)\n",
    "#save to csv\n",
    "sentimentprocess.to_csv('./sentdat/sentiment-big.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark NLP version 4.2.3\n",
      "Apache Spark version: 3.3.1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"Spark NLP version\", sparknlp.version())\n",
    "print(\"Apache Spark version:\", spark.version)\n",
    "\n",
    "#model = NerDLModel.pretrained('ner_dl')\n",
    "\n",
    "# doc_df=documentAssembler.transform(df)\n",
    "# doc_df.show()\n",
    "#has been transformed into a text, vector column\n",
    "#https://nlp.johnsnowlabs.com/2022/09/06/finclf_bert_sentiment_en.html\n",
    "\n",
    "# sort data by language? use different models for different languages?\n",
    "#train own model for english with amazon reviews?\n",
    "\n",
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol('text') \\\n",
    "    .setOutputCol('document')\n",
    "\n",
    "sentence_detector = SentenceDetector() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"sentence\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols(['sentence']) \\\n",
    "    .setOutputCol('token')\n",
    "\n",
    "# normalizer = Normalizer() \\\n",
    "#     .setInputCols(['token']) \\\n",
    "#     .setOutputCol('normalized') \\\n",
    "\n",
    "lemmatizer = Lemmatizer()\\\n",
    "    .setInputCols(['token'])\\\n",
    "    .setOutputCol('lemma')\\\n",
    "  .setDictionary(\"./sentdat/lemmas_small.txt\", key_delimiter=\"->\", value_delimiter=\"\\t\")\n",
    "#! wget -N https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/lemma-corpus-small/lemmas_small.txt -P /tmp\n",
    "SentimentDetector = sentiment.SentimentDetector() \\\n",
    "    .setInputCols(['lemma', 'sentence'])\\\n",
    "    .setOutputCol('sentiment_score')\\\n",
    "    .setDictionary('./sentdat/sentiment-big.csv', ',')\\\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    document_assembler, \n",
    "    sentence_detector,\n",
    "    tokenizer,\n",
    "    lemmatizer,\n",
    "    SentimentDetector\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget -N https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/sentiment-corpus/default-sentiment-dict.txt -P ./sentdat \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget -N https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/lemma-corpus-small/lemmas_small.txt -P ./sentdat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf = pipeline.fit(df).transform(df)\n",
    "#getting a new df with sentiment score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+--------------------+--------------------+--------------------+--------------------+---------------+\n",
      "|                text|             price|            document|            sentence|               token|               lemma|sentiment_score|\n",
      "+--------------------+------------------+--------------------+--------------------+--------------------+--------------------+---------------+\n",
      "| Chicago Med Seas...|394.69000244140625|[{document, 0, 14...|[{document, 1, 64...|[{token, 1, 7, Ch...|[{token, 1, 7, Ch...|       positive|\n",
      "| Roberto Guilherm...|394.69000244140625|[{document, 0, 60...|[{document, 1, 13...|[{token, 1, 7, Ro...|[{token, 1, 7, Ro...|       positive|\n",
      "+--------------------+------------------+--------------------+--------------------+--------------------+--------------------+---------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "newdf = newdf.withColumn(\"sentiment_score\", concat_ws(\",\", \"sentiment_score.result\"))\n",
    "newdf.show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positives 77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 61:==============================================>         (10 + 2) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negatives 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"positives\", newdf.filter(col('sentiment_score') == 'positive').count())\n",
    "print(\"negatives\", newdf.filter(col('sentiment_score') == 'negative').count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('am111_venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f57737e6d1a902c43fc81f11403a4faf60173bd712443272aa63091abb3c628b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
