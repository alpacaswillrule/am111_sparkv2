{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Usuage:\n",
    "1.scp this file, requirements.txt, a model called model_dl inside models folder, and sentdat folder to the cluster, and the dockerfile and\n",
    "training_fin_classfier.py to the cluster. \n",
    "Could scp everything but will take far longer, just scp everything but the parquet and make an empty parquet dir on the cluster.\n",
    "2.make an empty dir called articlespar.parquet on the cluster\n",
    "3.sudo build/run the dockerfile with -e PYTHONFILETORUN=./sentiment_long_optim.py, arguments go before the image name in run cmd\n",
    "if running locally just build and run dockerfile but with additional arguments of (-e AWS_ACCESS_KEY_ID= -e AWS_SECRET_ACCESS_KEY=)\n",
    "'''\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, IntegerType\n",
    "import sparknlp\n",
    "spark = sparknlp.start() \n",
    "# sparknlp.start(gpu=True) >> for training on GPU\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from pyspark.ml import Pipeline\n",
    "from langdetect import detect\n",
    "from pyspark.sql.functions import col, lit, concat_ws\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "import requests\n",
    "from warcio import ArchiveIterator\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "import yfinance as yf\n",
    "import boto3\n",
    "import botocore\n",
    "import random\n",
    "import sys \n",
    "import numpy as np\n",
    "#PARAMETERS\n",
    "path_dl_model = './models/model_dl'\n",
    "batch_size_max = sys.maxsize -1\n",
    "num_records_percrawl = 20 #number of recors to attempt to extract from each crawl\n",
    "ticker = 'SPY'\n",
    "#read in financewordlist.csv into the list\n",
    "wordlist = pd.read_csv('./sentdat/topics.csv', header=None)[0].tolist()\n",
    "wordlist.extend(yf.Ticker(ticker).info['longName'].split())\n",
    "number_warcs_to_analyze = 10 #number of warcs to perform sentiment analysis on, goes from most reccent to farther back onse\n",
    "\n",
    "#CREATING THE PIPELINE FOR LATER\n",
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol('text') \\\n",
    "    .setOutputCol('document')\n",
    "\n",
    "sentence_detector = SentenceDetector() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"sentence\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols(['sentence']) \\\n",
    "    .setOutputCol('token')\n",
    "\n",
    "lemmatizer = Lemmatizer()\\\n",
    "    .setInputCols(['token'])\\\n",
    "    .setOutputCol('lemma')\\\n",
    "  .setDictionary(\"./sentdat/lemmas_small.txt\", key_delimiter=\"->\", value_delimiter=\"\\t\")\n",
    "SentimentDetector = sentiment.SentimentDetector() \\\n",
    "    .setInputCols(['lemma', 'sentence'])\\\n",
    "    .setOutputCol('sentiment_score')\\\n",
    "    .setDictionary('./sentdat/sentiment-big.csv', ',')\\\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    document_assembler, \n",
    "    sentence_detector,\n",
    "    tokenizer,\n",
    "    lemmatizer,\n",
    "    SentimentDetector\n",
    "])\n",
    "###GETTING WARC FILE NAMES FROM S3, GRABBING A RANDOM SAMPLE OF THEM\n",
    "s3 = boto3.resource('s3')\n",
    "my_bucket = s3.Bucket('commoncrawl')\n",
    "warcs = []\n",
    "for object in my_bucket.objects.filter(Prefix='crawl-data/CC-NEWS/'):\n",
    "    if object.key.endswith('.warc.gz'):\n",
    "        warcs.append(object.key)\n",
    "\n",
    "warcs = warcs[-(number_warcs_to_analyze+10):-10]\n",
    "\n",
    "for index, warc in enumerate(warcs):\n",
    "    warcs[index] = 'https://data.commoncrawl.org/' + warc\n",
    "\n",
    "#function to convert time from commoncrawl format to y-m-d\n",
    "def convert_header_date(date):\n",
    "    return time.strftime('%Y-%m-%d', time.strptime(date, '%Y-%m-%dT%H:%M:%SZ'))\n",
    "\n",
    "\n",
    "#obtaining stock data from yahoo finance from 2019 to current date.\n",
    "currentdate = time.strftime(\"%Y-%m-%d\")\n",
    "stockdata = yf.download(ticker, start='2010-01-01', end=currentdate)['Adj Close']\n",
    "\n",
    "#creating scehma to store text and prices\n",
    "data = StructType([\\\n",
    "  StructField(\"text\", StringType(), True),\n",
    "    StructField(\"price\", StringType(), True),\n",
    "    StructField(\"date\", StringType(), True)  \n",
    "]\n",
    ")\n",
    "\n",
    "FINDMODEL = PipelineModel.load(path_dl_model)\n",
    "\n",
    "#function to drop non-finance articles\n",
    "def drop_nonfinance_articles(df):\n",
    "  df = FINDMODEL.transform(df)\n",
    "  df = df.withColumn('finance', df['financial_model_pred.result'].getItem(0).cast('float'))\n",
    "  df = df.filter(df['finance'] == 1.0)\n",
    "  return df\n",
    "\n",
    "batching_done = False\n",
    "pausing_index = 0\n",
    "while batching_done == False: \n",
    "  # creating the main rdd to store the data\n",
    "  list_of_rows_batch = []\n",
    "  rows_batch_len = 0\n",
    "  recordsfetched = 0\n",
    "  failures = 0\n",
    "  datelist = []\n",
    "\n",
    "  for index, warc_url in enumerate(warcs):\n",
    "      response = requests.get(warc_url, stream=True)\n",
    "      if response.ok!=True:\n",
    "          raise Exception(\"Error downloading WARC file\")\n",
    "      records = ArchiveIterator(response.raw, arc2warc=True)\n",
    "      #what this should do is write each record's plaintexxt to a csv file\n",
    "      for record in records:\n",
    "          if record.rec_type == 'response':\n",
    "              try: \n",
    "                  html = record.content_stream().read() .decode('utf-8')\n",
    "                  plaintext = BeautifulSoup(html, 'lxml').get_text()\n",
    "                  plaintext = re.sub(r'\\s+', ' ', plaintext)\n",
    "                  plaintext = re.sub(r'[^a-zA-Z0-9\\s]', '', plaintext).lower()\n",
    "\n",
    "                  #obtains plaintext from the html\n",
    "                  if detect(plaintext) == 'en' and len(plaintext) > 150:  \n",
    "                      date = record.rec_headers.get_header('WARC-Date')\n",
    "                      date = convert_header_date(date)\n",
    "                      # append the plaintext and price to the batch\n",
    "                      if date in stockdata.index:\n",
    "                          datelist.append(date)\n",
    "                          list_of_rows_batch.append({'text':plaintext, 'price':float(stockdata[date]), 'date':date})\n",
    "                          recordsfetched += 1\n",
    "                          rows_batch_len += 1\n",
    "                      else:\n",
    "                          print('date not in stockdata',date)\n",
    "                          #likely a weekend or holiday, so we will just skip the entire warc\n",
    "                          break\n",
    "                  else:\n",
    "                      recordsfetched += 1                          \n",
    "              except:\n",
    "                  recordsfetched += 1  # because if the entire warc file is not in english or wrong date, we still want to move on to the next one\n",
    "                  failures += 1\n",
    "                  #print(\"attempt record: \", record.rec_headers.get_header('WARC-Target-URI'), \" failed\")\n",
    "                  pass\n",
    "\n",
    "          if rows_batch_len >= batch_size_max:\n",
    "              print(\"batch size max reached\")\n",
    "              pausing_index = index\n",
    "\n",
    "          if recordsfetched >= num_records_percrawl:\n",
    "              recordsfetched = 0\n",
    "              print(\"warc done\")\n",
    "              break\n",
    "\n",
    "      #finishing up for the last batch in it wasn't full and num batches wasnt maxed out.\n",
    "  if rows_batch_len > 0:\n",
    "      datelist = np.unique(datelist)\n",
    "      newdatelist = []\n",
    "      for date in datelist:\n",
    "         newdatelist.append(str(date)) #TODO ADJUSTED THIS\n",
    "      datelist = newdatelist\n",
    "      print(\"done one batch, size: \", rows_batch_len)\n",
    "      rows_batch_len = 0\n",
    "\n",
    "  if pausing_index != 0:\n",
    "      warcs = warcs[pausing_index:]\n",
    "      pausing_index = 0\n",
    "  else:\n",
    "      batching_done = True\n",
    "  \n",
    "\n",
    "  print(\"failures: \", failures)\n",
    "\n",
    "  #now split list_of_rows_batch by dates in datelist\n",
    "  \n",
    "  list_of_lists_freach_date = [] #TODO adjust this\n",
    "  for date in datelist:\n",
    "      list_of_lists_freach_date.append([row for row in list_of_rows_batch if row['date'] == date])\n",
    "  \n",
    "    \n",
    "  sentscores = []\n",
    "  finacial_data = []\n",
    "\n",
    "  for index, one_date_lst in enumerate(list_of_lists_freach_date):\n",
    "    df = spark.createDataFrame(one_date_lst, schema=data)\n",
    "    numarticles = len(one_date_lst)\n",
    "    #dropping non-finance articles\n",
    "    df = drop_nonfinance_articles(df)\n",
    "    #drop unessecary columns created from dropping non-finance articles\n",
    "    cols = df.columns\n",
    "    for item in ['text', 'price', 'date']:\n",
    "        cols.remove(item)\n",
    "    df = df.drop(*cols)\n",
    "\n",
    "    df = pipeline.fit(df).transform(df)\n",
    "\n",
    "    df = df.withColumn(\"sentiment_score\", concat_ws(\",\", \"sentiment_score.result\"))\n",
    "\n",
    "    positives= df.filter(col('sentiment_score') == 'positive').count()\n",
    "    negatives = numarticles - positives\n",
    "    print(\"total positive and negatives for \", date[index])\n",
    "    print(\"positives\", positives)\n",
    "    print(\"negatives\", negatives)\n",
    "    if negatives == 0:\n",
    "      sentscores.append(positives)\n",
    "    sentscores.append(positives/negatives)\n",
    "    finacial_data.append(float(stockdata[date[index]]))\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "x = np.arange(len(finacial_data))\n",
    "plt.plot(x, finacial_data)\n",
    "plt.show()\n",
    "plt.plot(x, sentscores)\n",
    "plt.show()\n",
    "plt.plot(x, finacial_data)\n",
    "plt.plot(x, sentscores)\n",
    "plt.show()\n",
    "plt.savefig('sentiment.png')\n",
    "\n",
    "print(sentscores)\n",
    "print(finacial_data)\n",
    "print(datelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(datelist[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_date_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_lists_freach_date[:3]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "am111_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f57737e6d1a902c43fc81f11403a4faf60173bd712443272aa63091abb3c628b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
