{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#scp -i \"/home/johan/Johan_key.pem\" -o StrictHostKeyChecking=accept-new -r \"am111_spark\" ec2-user@ec2-54-221-60-110.compute-1.amazonaws.com:/home/ec2-user\n",
    "#ssh -i \"Johan_key.pem\" ec2-user@ec2-3-82-37-175.compute-1.amazonaws.com\n",
    "\n",
    "#ssh -i \"Johan_key.pem\" ec2-user@ec2-54-160-226-58.compute-1.amazonaws.com\n",
    "\n",
    "#scp -i \"/home/johan/Johan_key.pem\" -r ec2-user@ec2-184-72-157-243.compute-1.amazonaws.com:/home/ec2-user/am111_spark/models/dl_model_160000_1_1670974311.6058211 ./ \n",
    "\n",
    "\n",
    "#could put this into bootstrap shell script or smth to transfer the data. Or just upload it directly.\n",
    "\n",
    "#REMEBER TO USE ec2-user instead of root or ubuntu when using ubuntu\n",
    "\n",
    "\n",
    "#so we ssh onto the emr, and then scp our shit over, then built the dockerfile, then run the dockerfile.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improvments to make:  make running the script simpler? Perhaps get a single shell script to do everyting? \n",
    "Use boto3 to upload everything to cluster rather than shell scripting? Can you use boto3 to upload shit to master node? Or store all the scripts on s3, and use boto3 to upload them. \n",
    "\n",
    "Also: in spark-long-optim, make one dataframe for all the dates, and a seperate column that is associated with dates. Make a duplicate list of same length as the articles with the chronological dates? (or at least labeled date1, date2, etc).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "import os\n",
    "import paramiko\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export AWS_PROFILE=personal\n",
    "#FOR COMMAND LINE ARGUMENTS ON THIS PROFILE\n",
    "#INSTANCE PROFILE FOR EMR IS emr-all\n",
    "session = boto3.Session(profile_name='personal')\n",
    "\n",
    "emr = session.client('emr', region_name='us-east-1')\n",
    "# create s3 client\n",
    "s3 = session.resource('s3', region_name='us-east-1')\n",
    "# Create a cluster with the default configurations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_bucket = s3.Bucket('commoncrawl')\n",
    "warcs = []\n",
    "for object in my_bucket.objects.filter(Prefix='crawl-data/CC-NEWS/'):\n",
    "    if object.key.endswith('.warc.gz'):\n",
    "        warcs.append(object.key)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aws-cloudtrail-logs-012609303642-cf34e868\n",
      "aws-logs-012609303642-us-east-2\n",
      "johan-spark-commoncrawl-logs\n"
     ]
    }
   ],
   "source": [
    "#checking if correct aws accnt by printing buckets\n",
    "for bucket in s3.buckets.all():\n",
    "    print(bucket.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# createa a bucket for the logs\n",
    "bucket = s3.create_bucket(Bucket='johan-spark-commoncrawl-logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS IS MOST IMPORTANT CELL IT LAUNCHES CLUSTERS\n",
    "# Create a cluster with the default configurations \n",
    "dockerami = 'TODO'\n",
    "response = emr.run_job_flow(\n",
    "\n",
    "    Name='sentiment_analysios',\n",
    "    LogUri='s3://commoncrawl-logs/',\n",
    "    ReleaseLabel='emr-6.9.0',\n",
    "    Instances={\n",
    "        'InstanceGroups': [\n",
    "            {\n",
    "                'Name': 'Master nodes',\n",
    "                'Market': 'ON_DEMAND',\n",
    "                'InstanceRole': 'MASTER',\n",
    "                'InstanceType': 'm5.8xlarge',\n",
    "                'InstanceCount': 1,\n",
    "                'EbsConfiguration': {\n",
    "                    'EbsBlockDeviceConfigs': [\n",
    "                        {\n",
    "                            'VolumeSpecification': {\n",
    "                                'SizeInGB': 128,\n",
    "                                'VolumeType': 'gp2'\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "              },\n",
    "            {\n",
    "                'Name': 'worker nodes',\n",
    "                'Market': 'SPOT',\n",
    "                'InstanceRole': 'CORE',\n",
    "                'InstanceType': 'c5.4xlarge',\n",
    "                'InstanceCount': 1,\n",
    "            }\n",
    "        ],\n",
    "        'KeepJobFlowAliveWhenNoSteps': True,\n",
    "        'TerminationProtected': False,\n",
    "        'Ec2KeyName': 'Johan_key',\n",
    "    },\n",
    "    VisibleToAllUsers = True,\n",
    "    JobFlowRole='EMR_EC2_DefaultRole',\n",
    "    ServiceRole='EMR_DefaultRole',\n",
    "    Applications=[\n",
    "        {\n",
    "            'Name': 'Spark'\n",
    "        },\n",
    "    ]        \n",
    ",BootstrapActions=[ \n",
    "        {\n",
    "            'Name': 'Packages setup',\n",
    "            'ScriptBootstrapAction':\n",
    "                {\n",
    "                'Path': 's3://johan-spark-scripts/dockerbootstrap.sh'\n",
    "                }\n",
    "        }]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'JobFlowId': 'j-2ILTBCDXZG4OX', 'ClusterArn': 'arn:aws:elasticmapreduce:us-east-1:012609303642:cluster/j-2ILTBCDXZG4OX', 'ResponseMetadata': {'RequestId': '93d66eb8-08ab-4593-b416-99538ac4c6c8', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '93d66eb8-08ab-4593-b416-99538ac4c6c8', 'content-type': 'application/x-amz-json-1.1', 'content-length': '118', 'date': 'Thu, 15 Dec 2022 20:44:28 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING\n"
     ]
    }
   ],
   "source": [
    "#get cluster id\n",
    "cluster_id = response['JobFlowId']\n",
    "#get cluster status\n",
    "cluster_status = emr.describe_cluster(ClusterId=cluster_id)['Cluster']['Status']['State']\n",
    "print(cluster_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172.31.27.88\n"
     ]
    }
   ],
   "source": [
    "#use cluster id to get public dns\n",
    "clusternoton = False\n",
    "while clusternoton == False:\n",
    "  emr_list_instance_rep = emr.list_instances(\n",
    "          ClusterId=cluster_id,\n",
    "          InstanceGroupTypes=[\n",
    "              'MASTER',\n",
    "          ],\n",
    "          InstanceStates=[\n",
    "              'RUNNING',\n",
    "          ]\n",
    "      )\n",
    "  try:\n",
    "    print(emr_list_instance_rep['Instances'][0]['PrivateIpAddress'])\n",
    "    clusternoton = True\n",
    "  except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add step to cluster\n",
    "# need to first upload stuff to the cluster, which can be done with scp.\n",
    "#then chmod the script?\\\n",
    "#usr/bin/python3: can't open file '/home/ec2-user/am111_spark/training_fin_classfier.py': [Errno 13] Permission denied\n",
    "#this doesnt work, why? will do it manually for now.\n",
    "\n",
    "#ignore this code, just do it manually this is bugged\n",
    "\n",
    "response = emr.add_job_flow_steps(\n",
    "    JobFlowId=cluster_id,\n",
    "    Steps=[\n",
    "        {\n",
    "            'Name': 'bootstrap',\n",
    "            'ActionOnFailure': 'CONTINUE',\n",
    "            'HadoopJarStep': {\n",
    "                'Jar': 'command-runner.jar',\n",
    "                'Args': [\n",
    "                    'sudo',\n",
    "                    'pip3',\n",
    "                    'install',\n",
    "                    '-r',\n",
    "                    '/home/ec2-user/am111_spark/requirements.txt',\n",
    "                ]\n",
    "                 }\n",
    "        },\n",
    "        {\n",
    "            'Name': 'spark job',\n",
    "            'ActionOnFailure': 'CONTINUE',\n",
    "            'HadoopJarStep': {\n",
    "                'Jar': 'command-runner.jar',\n",
    "                'Args': [\n",
    "                    'python',\n",
    "                    '/home/ec2-user/am111_spark/training_fin_classfier.py',\n",
    "\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '1bebfcf8-42db-41a5-8775-062eefec44d5',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '1bebfcf8-42db-41a5-8775-062eefec44d5',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '0',\n",
       "   'date': 'Sat, 10 Dec 2022 21:28:51 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#terminate the cluster\n",
    "emr.terminate_job_flows(JobFlowIds=[cluster_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check cluster status every 20 seconds and print\n",
    "cluster_status = emr.describe_cluster(ClusterId=cluster_id)['Cluster']['Status']['State']\n",
    "while cluster_status != 'TERMINATED' or cluster_status != 'TERMINATED_WITH_ERRORS':\n",
    "    cluster_status = emr.describe_cluster(ClusterId=cluster_id)['Cluster']['Status']['State']\n",
    "    print(cluster_status)\n",
    "    time.sleep(20)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1 (v3.11.1:a7a450f84a, Dec  6 2022, 15:24:06) [Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1a1af0ee75eeea9e2e1ee996c87e7a2b11a0bebd85af04bb136d915cefc0abce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
