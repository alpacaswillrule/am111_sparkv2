{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#scp -i \"/home/johan/Johan_key.pem\" -o StrictHostKeyChecking=accept-new -r \"am111_spark\" ec2-user@ec2-184-72-157-243.compute-1.amazonaws.com:/home/ec2-user\n",
    "#ssh -i \"Johan_key.pem\" ec2-user@ec2-3-82-37-175.compute-1.amazonaws.com\n",
    "\n",
    "#ssh -i \"Johan_key.pem\" ec2-user@ec2-54-160-226-58.compute-1.amazonaws.com\n",
    "\n",
    "#scp -i \"/home/johan/Johan_key.pem\" -r ec2-user@ec2-184-72-157-243.compute-1.amazonaws.com:/home/ec2-user/am111_spark/models/ ./ \n",
    "\n",
    "\n",
    "#could put this into bootstrap shell script or smth to transfer the data. Or just upload it directly.\n",
    "\n",
    "#REMEBER TO USE ec2-user instead of root or ubuntu when using ubuntu\n",
    "\n",
    "\n",
    "#so we ssh onto the emr, and then scp our shit over, then built the dockerfile, then run the dockerfile.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "import os\n",
    "import paramiko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export AWS_PROFILE=personal\n",
    "#FOR COMMAND LINE ARGUMENTS ON THIS PROFILE\n",
    "#INSTANCE PROFILE FOR EMR IS emr-all\n",
    "session = boto3.Session(profile_name='personal')\n",
    "\n",
    "emr = session.client('emr', region_name='us-east-1')\n",
    "# create s3 client\n",
    "s3 = session.resource('s3', region_name='us-east-1')\n",
    "# Create a cluster with the default configurations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aws-cloudtrail-logs-012609303642-cf34e868\n",
      "aws-logs-012609303642-us-east-2\n",
      "johan-spark-commoncrawl-logs\n"
     ]
    }
   ],
   "source": [
    "#checking if correct aws accnt by printing buckets\n",
    "for bucket in s3.buckets.all():\n",
    "    print(bucket.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# createa a bucket for the logs\n",
    "bucket = s3.create_bucket(Bucket='johan-spark-commoncrawl-logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a cluster with the default configurations \n",
    "dockerami = 'TODO'\n",
    "response = emr.run_job_flow(\n",
    "\n",
    "    Name='train_fin_docker',\n",
    "    LogUri='s3://commoncrawl-logs/',\n",
    "    ReleaseLabel='emr-6.9.0',\n",
    "    Instances={\n",
    "        'InstanceGroups': [\n",
    "            {\n",
    "                'Name': 'Master nodes',\n",
    "                'Market': 'ON_DEMAND',\n",
    "                'InstanceRole': 'MASTER',\n",
    "                'InstanceType': 'm5.4xlarge',\n",
    "                'InstanceCount': 1,\n",
    "                'EbsConfiguration': {\n",
    "                    'EbsBlockDeviceConfigs': [\n",
    "                        {\n",
    "                            'VolumeSpecification': {\n",
    "                                'SizeInGB': 128,\n",
    "                                'VolumeType': 'gp2'\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "              },\n",
    "            {\n",
    "                'Name': 'worker nodes',\n",
    "                'Market': 'ON_DEMAND',\n",
    "                'InstanceRole': 'CORE',\n",
    "                'InstanceType': 'm5.xlarge',\n",
    "                'InstanceCount': 4,\n",
    "            }\n",
    "        ],\n",
    "        'KeepJobFlowAliveWhenNoSteps': True,\n",
    "        'TerminationProtected': False,\n",
    "        'Ec2KeyName': 'Johan_key',\n",
    "    },\n",
    "    VisibleToAllUsers = True,\n",
    "    JobFlowRole='EMR_EC2_DefaultRole',\n",
    "    ServiceRole='EMR_DefaultRole',\n",
    "    Applications=[\n",
    "        {\n",
    "            'Name': 'Spark'\n",
    "        },\n",
    "    ]        \n",
    ",BootstrapActions=[ \n",
    "        {\n",
    "            'Name': 'Packages setup',\n",
    "            'ScriptBootstrapAction':\n",
    "                {\n",
    "                'Path': 's3://johan-spark-scripts/dockerbootstrap.sh'\n",
    "                }\n",
    "        }]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'JobFlowId': 'j-2DEYW1GT1NBDX', 'ClusterArn': 'arn:aws:elasticmapreduce:us-east-1:012609303642:cluster/j-2DEYW1GT1NBDX', 'ResponseMetadata': {'RequestId': 'dda82bd3-b845-43c1-9379-121cf5c5c868', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'dda82bd3-b845-43c1-9379-121cf5c5c868', 'content-type': 'application/x-amz-json-1.1', 'content-length': '118', 'date': 'Tue, 13 Dec 2022 19:15:33 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING\n"
     ]
    }
   ],
   "source": [
    "#get cluster id\n",
    "cluster_id = response['JobFlowId']\n",
    "#get cluster status\n",
    "cluster_status = emr.describe_cluster(ClusterId=cluster_id)['Cluster']['Status']['State']\n",
    "print(cluster_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172.31.27.88\n"
     ]
    }
   ],
   "source": [
    "#use cluster id to get public dns\n",
    "clusternoton = False\n",
    "while clusternoton == False:\n",
    "  emr_list_instance_rep = emr.list_instances(\n",
    "          ClusterId=cluster_id,\n",
    "          InstanceGroupTypes=[\n",
    "              'MASTER',\n",
    "          ],\n",
    "          InstanceStates=[\n",
    "              'RUNNING',\n",
    "          ]\n",
    "      )\n",
    "  try:\n",
    "    print(emr_list_instance_rep['Instances'][0]['PrivateIpAddress'])\n",
    "    clusternoton = True\n",
    "  except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add step to cluster\n",
    "# need to first upload stuff to the cluster, which can be done with scp.\n",
    "#then chmod the script?\\\n",
    "#usr/bin/python3: can't open file '/home/ec2-user/am111_spark/training_fin_classfier.py': [Errno 13] Permission denied\n",
    "#this doesnt work, why? will do it manually for now.\n",
    "\n",
    "#ignore this code, just do it manually this is bugged\n",
    "\n",
    "response = emr.add_job_flow_steps(\n",
    "    JobFlowId=cluster_id,\n",
    "    Steps=[\n",
    "        {\n",
    "            'Name': 'bootstrap',\n",
    "            'ActionOnFailure': 'CONTINUE',\n",
    "            'HadoopJarStep': {\n",
    "                'Jar': 'command-runner.jar',\n",
    "                'Args': [\n",
    "                    'sudo',\n",
    "                    'pip3',\n",
    "                    'install',\n",
    "                    '-r',\n",
    "                    '/home/ec2-user/am111_spark/requirements.txt',\n",
    "                ]\n",
    "                 }\n",
    "        },\n",
    "        {\n",
    "            'Name': 'spark job',\n",
    "            'ActionOnFailure': 'CONTINUE',\n",
    "            'HadoopJarStep': {\n",
    "                'Jar': 'command-runner.jar',\n",
    "                'Args': [\n",
    "                    'python',\n",
    "                    '/home/ec2-user/am111_spark/training_fin_classfier.py',\n",
    "\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '1bebfcf8-42db-41a5-8775-062eefec44d5',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '1bebfcf8-42db-41a5-8775-062eefec44d5',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '0',\n",
       "   'date': 'Sat, 10 Dec 2022 21:28:51 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#terminate the cluster\n",
    "emr.terminate_job_flows(JobFlowIds=[cluster_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check cluster status every 20 seconds and print\n",
    "cluster_status = emr.describe_cluster(ClusterId=cluster_id)['Cluster']['Status']['State']\n",
    "while cluster_status != 'TERMINATED' or cluster_status != 'TERMINATED_WITH_ERRORS':\n",
    "    cluster_status = emr.describe_cluster(ClusterId=cluster_id)['Cluster']['Status']['State']\n",
    "    print(cluster_status)\n",
    "    time.sleep(20)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "am111_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f57737e6d1a902c43fc81f11403a4faf60173bd712443272aa63091abb3c628b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
