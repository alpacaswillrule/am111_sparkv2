{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/johanvlassak/.virtualenvs/spark/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/johanvlassak/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/johanvlassak/.ivy2/jars\n",
      "com.johnsnowlabs.nlp#spark-nlp_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-eb788c25-f0cb-4c24-a2f5-cc3b1ae94c3f;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.johnsnowlabs.nlp#spark-nlp_2.12;4.4.0 in central\n",
      "\tfound com.typesafe#config;1.4.2 in central\n",
      "\tfound org.rocksdb#rocksdbjni;6.29.5 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.828 in central\n",
      "\tfound com.github.universal-automata#liblevenshtein;3.0.0 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.0.0-beta-3 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.0.0-beta-3 in central\n",
      "\tfound com.google.code.gson#gson;2.3 in central\n",
      "\tfound it.unimi.dsi#fastutil;7.0.12 in central\n",
      "\tfound org.projectlombok#lombok;1.16.8 in central\n",
      "\tfound com.google.cloud#google-cloud-storage;2.16.0 in central\n",
      "\tfound com.google.guava#guava;31.1-jre in central\n",
      "\tfound com.google.guava#failureaccess;1.0.1 in central\n",
      "\tfound com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava in central\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.16 in central\n",
      "\tfound com.google.j2objc#j2objc-annotations;1.3 in central\n",
      "\tfound com.google.http-client#google-http-client;1.42.3 in central\n",
      "\tfound io.opencensus#opencensus-contrib-http-util;0.31.1 in central\n",
      "\tfound com.google.http-client#google-http-client-jackson2;1.42.3 in central\n",
      "\tfound com.google.http-client#google-http-client-gson;1.42.3 in central\n",
      "\tfound com.google.api-client#google-api-client;2.1.1 in central\n",
      "\tfound commons-codec#commons-codec;1.15 in central\n",
      "\tfound com.google.oauth-client#google-oauth-client;1.34.1 in central\n",
      "\tfound com.google.http-client#google-http-client-apache-v2;1.42.3 in central\n",
      "\tfound com.google.apis#google-api-services-storage;v1-rev20220705-2.0.0 in central\n",
      "\tfound com.google.code.gson#gson;2.10 in central\n",
      "\tfound com.google.cloud#google-cloud-core;2.9.0 in central\n",
      "\tfound com.google.auto.value#auto-value-annotations;1.10.1 in central\n",
      "\tfound com.google.cloud#google-cloud-core-http;2.9.0 in central\n",
      "\tfound com.google.http-client#google-http-client-appengine;1.42.3 in central\n",
      "\tfound com.google.api#gax-httpjson;0.105.1 in central\n",
      "\tfound com.google.cloud#google-cloud-core-grpc;2.9.0 in central\n",
      "\tfound io.grpc#grpc-core;1.51.0 in central\n",
      "\tfound com.google.api#gax;2.20.1 in central\n",
      "\tfound com.google.api#gax-grpc;2.20.1 in central\n",
      "\tfound io.grpc#grpc-alts;1.51.0 in central\n",
      "\tfound io.grpc#grpc-grpclb;1.51.0 in central\n",
      "\tfound org.conscrypt#conscrypt-openjdk-uber;2.5.2 in central\n",
      "\tfound io.grpc#grpc-protobuf;1.51.0 in central\n",
      "\tfound com.google.auth#google-auth-library-credentials;1.13.0 in central\n",
      "\tfound com.google.auth#google-auth-library-oauth2-http;1.13.0 in central\n",
      "\tfound com.google.api#api-common;2.2.2 in central\n",
      "\tfound javax.annotation#javax.annotation-api;1.3.2 in central\n",
      "\tfound io.opencensus#opencensus-api;0.31.1 in central\n",
      "\tfound io.grpc#grpc-context;1.51.0 in central\n",
      "\tfound com.google.api.grpc#proto-google-iam-v1;1.6.22 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.21.10 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.21.10 in central\n",
      "\tfound com.google.api.grpc#proto-google-common-protos;2.11.0 in central\n",
      "\tfound org.threeten#threetenbp;1.6.4 in central\n",
      "\tfound com.google.api.grpc#proto-google-cloud-storage-v2;2.16.0-alpha in central\n",
      "\tfound com.google.api.grpc#grpc-google-cloud-storage-v2;2.16.0-alpha in central\n",
      "\tfound com.google.api.grpc#gapic-google-cloud-storage-v2;2.16.0-alpha in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.14.1 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound io.grpc#grpc-api;1.51.0 in central\n",
      "\tfound io.grpc#grpc-auth;1.51.0 in central\n",
      "\tfound io.grpc#grpc-stub;1.51.0 in central\n",
      "\tfound org.checkerframework#checker-qual;3.28.0 in central\n",
      "\tfound com.google.api.grpc#grpc-google-iam-v1;1.6.22 in central\n",
      "\tfound io.grpc#grpc-protobuf-lite;1.51.0 in central\n",
      "\tfound com.google.android#annotations;4.1.1.4 in central\n",
      "\tfound org.codehaus.mojo#animal-sniffer-annotations;1.22 in central\n",
      "\tfound io.grpc#grpc-netty-shaded;1.51.0 in central\n",
      "\tfound io.perfmark#perfmark-api;0.26.0 in central\n",
      "\tfound io.grpc#grpc-googleapis;1.51.0 in central\n",
      "\tfound io.grpc#grpc-xds;1.51.0 in central\n",
      "\tfound io.opencensus#opencensus-proto;0.2.0 in central\n",
      "\tfound io.grpc#grpc-services;1.51.0 in central\n",
      "\tfound com.google.re2j#re2j;1.6 in central\n",
      "\tfound com.navigamez#greex;1.0 in central\n",
      "\tfound dk.brics.automaton#automaton;1.11-8 in central\n",
      "\tfound com.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.4 in central\n",
      ":: resolution report :: resolve 505ms :: artifacts dl 13ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.828 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.14.1 from central in [default]\n",
      "\tcom.github.universal-automata#liblevenshtein;3.0.0 from central in [default]\n",
      "\tcom.google.android#annotations;4.1.1.4 from central in [default]\n",
      "\tcom.google.api#api-common;2.2.2 from central in [default]\n",
      "\tcom.google.api#gax;2.20.1 from central in [default]\n",
      "\tcom.google.api#gax-grpc;2.20.1 from central in [default]\n",
      "\tcom.google.api#gax-httpjson;0.105.1 from central in [default]\n",
      "\tcom.google.api-client#google-api-client;2.1.1 from central in [default]\n",
      "\tcom.google.api.grpc#gapic-google-cloud-storage-v2;2.16.0-alpha from central in [default]\n",
      "\tcom.google.api.grpc#grpc-google-cloud-storage-v2;2.16.0-alpha from central in [default]\n",
      "\tcom.google.api.grpc#grpc-google-iam-v1;1.6.22 from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-cloud-storage-v2;2.16.0-alpha from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-common-protos;2.11.0 from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-iam-v1;1.6.22 from central in [default]\n",
      "\tcom.google.apis#google-api-services-storage;v1-rev20220705-2.0.0 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-credentials;1.13.0 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-oauth2-http;1.13.0 from central in [default]\n",
      "\tcom.google.auto.value#auto-value-annotations;1.10.1 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core;2.9.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core-grpc;2.9.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core-http;2.9.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-storage;2.16.0 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.10 from central in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.16 from central in [default]\n",
      "\tcom.google.guava#failureaccess;1.0.1 from central in [default]\n",
      "\tcom.google.guava#guava;31.1-jre from central in [default]\n",
      "\tcom.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava from central in [default]\n",
      "\tcom.google.http-client#google-http-client;1.42.3 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-apache-v2;1.42.3 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-appengine;1.42.3 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-gson;1.42.3 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-jackson2;1.42.3 from central in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;1.3 from central in [default]\n",
      "\tcom.google.oauth-client#google-oauth-client;1.34.1 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.21.10 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.21.10 from central in [default]\n",
      "\tcom.google.re2j#re2j;1.6 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#spark-nlp_2.12;4.4.0 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.4 from central in [default]\n",
      "\tcom.navigamez#greex;1.0 from central in [default]\n",
      "\tcom.typesafe#config;1.4.2 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.15 from central in [default]\n",
      "\tdk.brics.automaton#automaton;1.11-8 from central in [default]\n",
      "\tio.grpc#grpc-alts;1.51.0 from central in [default]\n",
      "\tio.grpc#grpc-api;1.51.0 from central in [default]\n",
      "\tio.grpc#grpc-auth;1.51.0 from central in [default]\n",
      "\tio.grpc#grpc-context;1.51.0 from central in [default]\n",
      "\tio.grpc#grpc-core;1.51.0 from central in [default]\n",
      "\tio.grpc#grpc-googleapis;1.51.0 from central in [default]\n",
      "\tio.grpc#grpc-grpclb;1.51.0 from central in [default]\n",
      "\tio.grpc#grpc-netty-shaded;1.51.0 from central in [default]\n",
      "\tio.grpc#grpc-protobuf;1.51.0 from central in [default]\n",
      "\tio.grpc#grpc-protobuf-lite;1.51.0 from central in [default]\n",
      "\tio.grpc#grpc-services;1.51.0 from central in [default]\n",
      "\tio.grpc#grpc-stub;1.51.0 from central in [default]\n",
      "\tio.grpc#grpc-xds;1.51.0 from central in [default]\n",
      "\tio.opencensus#opencensus-api;0.31.1 from central in [default]\n",
      "\tio.opencensus#opencensus-contrib-http-util;0.31.1 from central in [default]\n",
      "\tio.opencensus#opencensus-proto;0.2.0 from central in [default]\n",
      "\tio.perfmark#perfmark-api;0.26.0 from central in [default]\n",
      "\tit.unimi.dsi#fastutil;7.0.12 from central in [default]\n",
      "\tjavax.annotation#javax.annotation-api;1.3.2 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.28.0 from central in [default]\n",
      "\torg.codehaus.mojo#animal-sniffer-annotations;1.22 from central in [default]\n",
      "\torg.conscrypt#conscrypt-openjdk-uber;2.5.2 from central in [default]\n",
      "\torg.projectlombok#lombok;1.16.8 from central in [default]\n",
      "\torg.rocksdb#rocksdbjni;6.29.5 from central in [default]\n",
      "\torg.threeten#threetenbp;1.6.4 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcom.google.protobuf#protobuf-java-util;3.0.0-beta-3 by [com.google.protobuf#protobuf-java-util;3.21.10] in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.0.0-beta-3 by [com.google.protobuf#protobuf-java;3.21.10] in [default]\n",
      "\tcom.google.code.gson#gson;2.3 by [com.google.code.gson#gson;2.10] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   73  |   0   |   0   |   3   ||   70  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-eb788c25-f0cb-4c24-a2f5-cc3b1ae94c3f\n",
      "\tconfs: [default]\n",
      "\t1 artifacts copied, 69 already retrieved (283550kB/204ms)\n",
      "23/04/19 20:29:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/04/19 20:29:43 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './sentdat/topics.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m ticker \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mSPY\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     31\u001b[0m \u001b[39m#read in financewordlist.csv into the list\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m wordlist \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m'\u001b[39;49m\u001b[39m./sentdat/topics.csv\u001b[39;49m\u001b[39m'\u001b[39;49m, header\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtolist()\n\u001b[1;32m     33\u001b[0m wordlist\u001b[39m.\u001b[39mextend(yf\u001b[39m.\u001b[39mTicker(ticker)\u001b[39m.\u001b[39minfo[\u001b[39m'\u001b[39m\u001b[39mlongName\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39msplit())\n\u001b[1;32m     34\u001b[0m number_warcs_to_analyze \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m \u001b[39m#int(os.environ['NUMWARCS']) #number of warcs to perform sentiment analysis on, goes from most reccent to farther back onse\u001b[39;00m\n",
      "File \u001b[0;32m~/.virtualenvs/spark/lib/python3.11/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/.virtualenvs/spark/lib/python3.11/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    579\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.virtualenvs/spark/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m~/.virtualenvs/spark/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m   1662\u001b[0m     f,\n\u001b[1;32m   1663\u001b[0m     mode,\n\u001b[1;32m   1664\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1665\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1666\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1667\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1668\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1669\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1670\u001b[0m )\n\u001b[1;32m   1671\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.virtualenvs/spark/lib/python3.11/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    860\u001b[0m             handle,\n\u001b[1;32m    861\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    862\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    863\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    864\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    865\u001b[0m         )\n\u001b[1;32m    866\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './sentdat/topics.csv'"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, IntegerType\n",
    "import sparknlp\n",
    "spark = sparknlp.start() \n",
    "# sparknlp.start(gpu=True) >> for training on GPU\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from pyspark.ml import Pipeline\n",
    "from langdetect import detect\n",
    "from pyspark.sql.functions import col,udf, lit, concat_ws\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "import requests\n",
    "from warcio import ArchiveIterator\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "import yfinance as yf\n",
    "import boto3\n",
    "import random\n",
    "import sys \n",
    "import numpy as np\n",
    "import os\n",
    "#PARAMETERS\n",
    "path_dl_model = os.path.join(os.getcwd(), 'models/model_dl') \n",
    "batch_size_max = sys.maxsize -1\n",
    "num_records_percrawl = 10#int(os.environ['NUMRECORDS']) #number of recors to attempt to extract from each crawl\n",
    "ticker = 'SPY'\n",
    "\n",
    "number_warcs_to_analyze = 10 #int(os.environ['NUMWARCS']) #number of warcs to perform sentiment analysis on, goes from most reccent to farther back onse\n",
    "randomsample = 'n' #str(os.environ['RANSAMPLE']).lower() #Y or N, if Y, then it will take a random sample of warcs to analyze, if N, it will take the most recent warcs\n",
    "\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "my_bucket = s3.Bucket('commoncrawl')\n",
    "warcs = []\n",
    "for object in my_bucket.objects.filter(Prefix='crawl-data/CC-NEWS/'):\n",
    "    if object.key.endswith('.warc.gz'):\n",
    "        warcs.append(object.key)\n",
    "\n",
    "if randomsample == 'y':\n",
    "    warcs = random.sample(warcs, number_warcs_to_analyze)\n",
    "else:\n",
    "  warcs = warcs[-number_warcs_to_analyze:]\n",
    "\n",
    "for index, warc in enumerate(warcs):\n",
    "    warcs[index] = 'https://data.commoncrawl.org/' + warc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('starting to load model')\n",
    "FINDMODEL = PipelineModel.load(path_dl_model)\n",
    "print('model loaded')\n",
    "\n",
    "# Broadcast the model to avoid replication\n",
    "FINDMODEL_broadcast = spark.sparkContext.broadcast(FINDMODEL)\n",
    "\n",
    "# Create a user-defined function for the finance model\n",
    "@udf(returnType=FloatType())\n",
    "def is_finance(article_text):\n",
    "    local_model = FINDMODEL_broadcast.value\n",
    "    prediction = local_model.transform(article_text)\n",
    "    return float(prediction['financial_model_pred.result'][0])\n",
    "\n",
    "# Function to drop non-finance articles\n",
    "def drop_nonfinance_articles(df):\n",
    "    df = df.withColumn('finance', is_finance(df['article_text']))\n",
    "    df = df.filter(df['finance'] == 1.0)\n",
    "    return df\n",
    "\n",
    "#function to convert time from commoncrawl format to y-m-d\n",
    "def convert_header_date(date):\n",
    "    return time.strftime('%Y-%m-%d', time.strptime(date, '%Y-%m-%dT%H:%M:%SZ'))\n",
    "\n",
    "\n",
    "#obtaining stock data from yahoo finance from 2019 to current date.\n",
    "currentdate = time.strftime(\"%Y-%m-%d\")\n",
    "stockdata = yf.download(ticker, start='2010-01-01', end=currentdate)['Adj Close']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#READING IN THE WARCS\n",
    "from io import BytesIO\n",
    "\n",
    "def extract_plaintext_from_html(html_content):\n",
    "    \"\"\"Remove HTML tags from content and return plaintext.\"\"\"\n",
    "    plaintext = re.sub('<[^<]+?>', ' ', html_content)\n",
    "    return plaintext\n",
    "\n",
    "def parse_warc_record(record, stockdata):\n",
    "    \"\"\"Parse WARC record and return tuple with date and plaintext.\"\"\"\n",
    "    if record.rec_type == 'response':\n",
    "        url = record.rec_headers.get_header('WARC-Target-URI')\n",
    "        content_type = record.http_headers.get_header('Content-Type')\n",
    "        if content_type and 'text/html' in content_type:\n",
    "            content = record.content_stream().read().decode('utf-8', 'replace')\n",
    "            plaintext = extract_plaintext_from_html(content)\n",
    "            if detect(plaintext) == 'en' and len(plaintext) > 150:\n",
    "                date = record.rec_headers.get_header('WARC-Date')\n",
    "                date = convert_header_date(date)\n",
    "                if date in stockdata.index:\n",
    "                    return (date, plaintext)\n",
    "                else:\n",
    "                    print('date not in stockdata', date)\n",
    "    return None\n",
    "\n",
    "def parse_warc(warc_data, stockdata):\n",
    "    \"\"\"Parse WARC data and return list of tuples with dates and plaintexts.\"\"\"\n",
    "    records = []\n",
    "    with BytesIO(warc_data) as stream:\n",
    "        for record in ArchiveIterator(stream):\n",
    "            parsed_record = parse_warc_record(record, stockdata)\n",
    "            if parsed_record:\n",
    "                records.append(parsed_record)\n",
    "    return records\n",
    "\n",
    "def fetch_and_parse_warc(url, stockdata):\n",
    "    \"\"\"Fetch WARC data from URL and parse it to extract plaintext.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        warc_data = response.content\n",
    "        return parse_warc(warc_data, stockdata)\n",
    "    return []\n",
    "\n",
    "\n",
    "date_plaintexts = []\n",
    "for warc_url in warcs:\n",
    "    warc_records = fetch_and_parse_warc(warc_url, stockdata)\n",
    "    for index, record in enumerate(warc_records):\n",
    "        warc_records[index] = (record[0], record[1], float(stockdata[record[0]]))\n",
    "    date_plaintexts.extend(warc_records)\n",
    "\n",
    "#creating scehma to store text and prices\n",
    "data = StructType([\\\n",
    "  StructField(\"text\", StringType(), True),\n",
    "    StructField(\"price\", StringType(), True),\n",
    "    StructField(\"date\", StringType(), True)  \n",
    "]\n",
    ")\n",
    "\n",
    "\n",
    "#creating dataframe from schema\n",
    "df = spark.createDataFrame(date_plaintexts, schema=data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "ADD SPARKWARC AND STUFF TO REQUIREMENTS.TXT\n",
    "implement new dataframe import\n",
    "Re-work the imports\n",
    "Re-write the final analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort out the finance articles\n",
    "df = drop_nonfinance_articles(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#CREATING THE PIPELINE FOR LATER\n",
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol('text') \\\n",
    "    .setOutputCol('document')\n",
    "\n",
    "sentence_detector = SentenceDetector() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"sentence\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols(['sentence']) \\\n",
    "    .setOutputCol('token')\n",
    "\n",
    "lemmatizer = Lemmatizer()\\\n",
    "    .setInputCols(['token'])\\\n",
    "    .setOutputCol('lemma')\\\n",
    "  .setDictionary(\"./sentdat/lemmas_small.txt\", key_delimiter=\"->\", value_delimiter=\"\\t\")\n",
    "SentimentDetector = sentiment.SentimentDetector() \\\n",
    "    .setInputCols(['lemma', 'sentence'])\\\n",
    "    .setOutputCol('sentiment_score')\\\n",
    "    .setDictionary('./sentdat/sentiment-big.csv', ',')\\\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    document_assembler, \n",
    "    sentence_detector,\n",
    "    tokenizer,\n",
    "    lemmatizer,\n",
    "    SentimentDetector\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLOTTING CODE\n",
    "\n",
    "from pyspark.sql.functions import col, avg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "agg_df = df.groupBy(\"date\").agg(avg(\"sentiment_score\").alias(\"average_sentiment\"))\n",
    "\n",
    "# Convert the aggregated PySpark DataFrame to a Pandas DataFrame\n",
    "pd_agg_df = agg_df.toPandas()\n",
    "\n",
    "# Convert 'date' column to pandas datetime object for better plotting\n",
    "pd_agg_df['date'] = pd.to_datetime(pd_agg_df['date'])\n",
    "\n",
    "# Sort the DataFrame by date\n",
    "pd_agg_df = pd_agg_df.sort_values(by=\"date\")\n",
    "\n",
    "# Plot the average sentiment score as a function of date\n",
    "plt.plot(pd_agg_df[\"date\"], pd_agg_df[\"average_sentiment\"])\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Average Sentiment Score\")\n",
    "plt.title(\"Average Sentiment Score by Date\")\n",
    "plt.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1 (v3.11.1:a7a450f84a, Dec  6 2022, 15:24:06) [Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1982945563604bc15fa9a2895e34370c25ba4311e07392b427372a2d8dc81522"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
