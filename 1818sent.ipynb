{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/04 16:25:29 WARN Utils: Your hostname, DESKTOP-QC3P089 resolves to a loopback address: 127.0.1.1; using 192.168.147.38 instead (on interface eth0)\n",
      "23/05/04 16:25:29 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/home/johan/am111/am111_venv/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/johan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/johan/.ivy2/jars\n",
      "com.johnsnowlabs.nlp#spark-nlp_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-9a8537dc-d1dc-423c-a5b5-613d21824523;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.johnsnowlabs.nlp#spark-nlp_2.12;4.2.4 in central\n",
      "\tfound com.typesafe#config;1.4.2 in central\n",
      "\tfound org.rocksdb#rocksdbjni;6.29.5 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.828 in central\n",
      "\tfound com.github.universal-automata#liblevenshtein;3.0.0 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.0.0-beta-3 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.0.0-beta-3 in central\n",
      "\tfound com.google.code.gson#gson;2.3 in central\n",
      "\tfound it.unimi.dsi#fastutil;7.0.12 in central\n",
      "\tfound org.projectlombok#lombok;1.16.8 in central\n",
      "\tfound com.google.cloud#google-cloud-storage;2.15.0 in central\n",
      "\tfound com.google.guava#guava;31.1-jre in central\n",
      "\tfound com.google.guava#failureaccess;1.0.1 in central\n",
      "\tfound com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava in central\n",
      "\tfound com.google.j2objc#j2objc-annotations;1.3 in central\n",
      "\tfound com.google.http-client#google-http-client;1.42.3 in central\n",
      "\tfound io.opencensus#opencensus-contrib-http-util;0.31.1 in central\n",
      "\tfound com.google.http-client#google-http-client-jackson2;1.42.3 in central\n",
      "\tfound com.google.http-client#google-http-client-gson;1.42.3 in central\n",
      "\tfound com.google.api-client#google-api-client;2.0.1 in central\n",
      "\tfound com.google.oauth-client#google-oauth-client;1.34.1 in central\n",
      "\tfound com.google.http-client#google-http-client-apache-v2;1.42.3 in central\n",
      "\tfound com.google.apis#google-api-services-storage;v1-rev20220705-2.0.0 in central\n",
      "\tfound com.google.code.gson#gson;2.10 in central\n",
      "\tfound com.google.cloud#google-cloud-core;2.8.27 in central\n",
      "\tfound com.google.auto.value#auto-value-annotations;1.10 in central\n",
      "\tfound com.google.cloud#google-cloud-core-http;2.8.27 in central\n",
      "\tfound com.google.http-client#google-http-client-appengine;1.42.3 in central\n",
      "\tfound com.google.api#gax-httpjson;0.104.5 in central\n",
      "\tfound com.google.cloud#google-cloud-core-grpc;2.8.27 in central\n",
      "\tfound io.grpc#grpc-core;1.50.2 in central\n",
      "\tfound com.google.api#gax;2.19.5 in central\n",
      "\tfound com.google.api#gax-grpc;2.19.5 in central\n",
      "\tfound com.google.auth#google-auth-library-credentials;1.12.1 in central\n",
      "\tfound com.google.auth#google-auth-library-oauth2-http;1.12.1 in central\n",
      "\tfound com.google.api#api-common;2.2.2 in central\n",
      "\tfound javax.annotation#javax.annotation-api;1.3.2 in central\n",
      "\tfound io.opencensus#opencensus-api;0.31.1 in central\n",
      "\tfound io.grpc#grpc-context;1.50.2 in central\n",
      "\tfound com.google.api.grpc#proto-google-iam-v1;1.6.7 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.21.9 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.21.9 in central\n",
      "\tfound com.google.api.grpc#proto-google-common-protos;2.10.0 in central\n",
      "\tfound org.threeten#threetenbp;1.6.4 in central\n",
      "\tfound com.google.api.grpc#proto-google-cloud-storage-v2;2.15.0-alpha in central\n",
      "\tfound com.google.api.grpc#grpc-google-cloud-storage-v2;2.15.0-alpha in central\n",
      "\tfound io.grpc#grpc-protobuf;1.50.2 in central\n",
      "\tfound com.google.api.grpc#gapic-google-cloud-storage-v2;2.15.0-alpha in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.14.0 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound io.grpc#grpc-api;1.50.2 in central\n",
      "\tfound io.grpc#grpc-auth;1.50.2 in central\n",
      "\tfound io.grpc#grpc-stub;1.50.2 in central\n",
      "\tfound org.checkerframework#checker-qual;3.27.0 in central\n",
      "\tfound com.google.api.grpc#grpc-google-iam-v1;1.6.7 in central\n",
      "\tfound io.grpc#grpc-protobuf-lite;1.50.2 in central\n",
      "\tfound com.google.android#annotations;4.1.1.4 in central\n",
      "\tfound org.codehaus.mojo#animal-sniffer-annotations;1.22 in central\n",
      "\tfound io.grpc#grpc-alts;1.50.2 in central\n",
      "\tfound io.grpc#grpc-grpclb;1.50.2 in central\n",
      "\tfound org.conscrypt#conscrypt-openjdk-uber;2.5.2 in central\n",
      "\tfound io.grpc#grpc-netty-shaded;1.50.2 in central\n",
      "\tfound io.perfmark#perfmark-api;0.25.0 in central\n",
      "\tfound io.grpc#grpc-googleapis;1.50.2 in central\n",
      "\tfound io.grpc#grpc-xds;1.50.2 in central\n",
      "\tfound io.opencensus#opencensus-proto;0.2.0 in central\n",
      "\tfound io.grpc#grpc-services;1.50.2 in central\n",
      "\tfound com.google.re2j#re2j;1.6 in central\n",
      "\tfound com.navigamez#greex;1.0 in central\n",
      "\tfound dk.brics.automaton#automaton;1.11-8 in central\n",
      "\tfound com.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.4 in central\n",
      ":: resolution report :: resolve 888ms :: artifacts dl 23ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.828 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.14.0 from central in [default]\n",
      "\tcom.github.universal-automata#liblevenshtein;3.0.0 from central in [default]\n",
      "\tcom.google.android#annotations;4.1.1.4 from central in [default]\n",
      "\tcom.google.api#api-common;2.2.2 from central in [default]\n",
      "\tcom.google.api#gax;2.19.5 from central in [default]\n",
      "\tcom.google.api#gax-grpc;2.19.5 from central in [default]\n",
      "\tcom.google.api#gax-httpjson;0.104.5 from central in [default]\n",
      "\tcom.google.api-client#google-api-client;2.0.1 from central in [default]\n",
      "\tcom.google.api.grpc#gapic-google-cloud-storage-v2;2.15.0-alpha from central in [default]\n",
      "\tcom.google.api.grpc#grpc-google-cloud-storage-v2;2.15.0-alpha from central in [default]\n",
      "\tcom.google.api.grpc#grpc-google-iam-v1;1.6.7 from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-cloud-storage-v2;2.15.0-alpha from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-common-protos;2.10.0 from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-iam-v1;1.6.7 from central in [default]\n",
      "\tcom.google.apis#google-api-services-storage;v1-rev20220705-2.0.0 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-credentials;1.12.1 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-oauth2-http;1.12.1 from central in [default]\n",
      "\tcom.google.auto.value#auto-value-annotations;1.10 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core;2.8.27 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core-grpc;2.8.27 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core-http;2.8.27 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-storage;2.15.0 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.10 from central in [default]\n",
      "\tcom.google.guava#failureaccess;1.0.1 from central in [default]\n",
      "\tcom.google.guava#guava;31.1-jre from central in [default]\n",
      "\tcom.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava from central in [default]\n",
      "\tcom.google.http-client#google-http-client;1.42.3 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-apache-v2;1.42.3 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-appengine;1.42.3 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-gson;1.42.3 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-jackson2;1.42.3 from central in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;1.3 from central in [default]\n",
      "\tcom.google.oauth-client#google-oauth-client;1.34.1 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.21.9 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.21.9 from central in [default]\n",
      "\tcom.google.re2j#re2j;1.6 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#spark-nlp_2.12;4.2.4 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.4 from central in [default]\n",
      "\tcom.navigamez#greex;1.0 from central in [default]\n",
      "\tcom.typesafe#config;1.4.2 from central in [default]\n",
      "\tdk.brics.automaton#automaton;1.11-8 from central in [default]\n",
      "\tio.grpc#grpc-alts;1.50.2 from central in [default]\n",
      "\tio.grpc#grpc-api;1.50.2 from central in [default]\n",
      "\tio.grpc#grpc-auth;1.50.2 from central in [default]\n",
      "\tio.grpc#grpc-context;1.50.2 from central in [default]\n",
      "\tio.grpc#grpc-core;1.50.2 from central in [default]\n",
      "\tio.grpc#grpc-googleapis;1.50.2 from central in [default]\n",
      "\tio.grpc#grpc-grpclb;1.50.2 from central in [default]\n",
      "\tio.grpc#grpc-netty-shaded;1.50.2 from central in [default]\n",
      "\tio.grpc#grpc-protobuf;1.50.2 from central in [default]\n",
      "\tio.grpc#grpc-protobuf-lite;1.50.2 from central in [default]\n",
      "\tio.grpc#grpc-services;1.50.2 from central in [default]\n",
      "\tio.grpc#grpc-stub;1.50.2 from central in [default]\n",
      "\tio.grpc#grpc-xds;1.50.2 from central in [default]\n",
      "\tio.opencensus#opencensus-api;0.31.1 from central in [default]\n",
      "\tio.opencensus#opencensus-contrib-http-util;0.31.1 from central in [default]\n",
      "\tio.opencensus#opencensus-proto;0.2.0 from central in [default]\n",
      "\tio.perfmark#perfmark-api;0.25.0 from central in [default]\n",
      "\tit.unimi.dsi#fastutil;7.0.12 from central in [default]\n",
      "\tjavax.annotation#javax.annotation-api;1.3.2 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.27.0 from central in [default]\n",
      "\torg.codehaus.mojo#animal-sniffer-annotations;1.22 from central in [default]\n",
      "\torg.conscrypt#conscrypt-openjdk-uber;2.5.2 from central in [default]\n",
      "\torg.projectlombok#lombok;1.16.8 from central in [default]\n",
      "\torg.rocksdb#rocksdbjni;6.29.5 from central in [default]\n",
      "\torg.threeten#threetenbp;1.6.4 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcom.google.protobuf#protobuf-java-util;3.0.0-beta-3 by [com.google.protobuf#protobuf-java-util;3.21.9] in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.0.0-beta-3 by [com.google.protobuf#protobuf-java;3.21.9] in [default]\n",
      "\tcom.google.code.gson#gson;2.3 by [com.google.code.gson#gson;2.10] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   71  |   0   |   0   |   3   ||   68  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-9a8537dc-d1dc-423c-a5b5-613d21824523\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 68 already retrieved (0kB/16ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/04 16:25:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#dockerfile test edit\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, IntegerType\n",
    "import sparknlp\n",
    "spark = sparknlp.start() \n",
    "# sparknlp.start(gpu=True) >> for training on GPU\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from pyspark.ml import Pipeline\n",
    "from langdetect import detect\n",
    "from pyspark.sql.functions import col,udf, lit, concat_ws\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "import requests\n",
    "from warcio import ArchiveIterator\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "import yfinance as yf\n",
    "import boto3\n",
    "import random\n",
    "import sys \n",
    "import numpy as np\n",
    "import os\n",
    "#PARAMETERS\n",
    "path_dl_model = './models/model_dl'\n",
    "batch_size_max = sys.maxsize -1\n",
    "num_records_percrawl = 10#int(os.environ['NUMRECORDS']) #number of recors to attempt to extract from each crawl\n",
    "ticker = 'SPY'\n",
    "\n",
    "number_warcs_to_analyze = 30 #int(os.environ['NUMWARCS']) #number of warcs to perform sentiment analysis on, goes from most reccent to farther back onse\n",
    "randomsample = 'n' #str(os.environ['RANSAMPLE']).lower() #Y or N, if Y, then it will take a random sample of warcs to analyze, if N, it will take the most recent warcs\n",
    "\n",
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol('text') \\\n",
    "    .setOutputCol('document')\n",
    "\n",
    "sentence_detector = SentenceDetector() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"sentence\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols(['sentence']) \\\n",
    "    .setOutputCol('token')\n",
    "\n",
    "lemmatizer = Lemmatizer()\\\n",
    "    .setInputCols(['token'])\\\n",
    "    .setOutputCol('lemma')\\\n",
    "  .setDictionary(\"./sentdat/lemmas_small.txt\", key_delimiter=\"->\", value_delimiter=\"\\t\")\n",
    "SentimentDetector = sentiment.SentimentDetector() \\\n",
    "    .setInputCols(['lemma', 'sentence'])\\\n",
    "    .setOutputCol('sentiment_score')\\\n",
    "    .setDictionary('./sentdat/sentiment-big.csv', ',')\\\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    document_assembler, \n",
    "    sentence_detector,\n",
    "    tokenizer,\n",
    "    lemmatizer,\n",
    "    SentimentDetector\n",
    "])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "my_bucket = s3.Bucket('commoncrawl')\n",
    "warcs = []\n",
    "for object in my_bucket.objects.filter(Prefix='crawl-data/CC-NEWS/'):\n",
    "    if object.key.endswith('.warc.gz'):\n",
    "        warcs.append(object.key)\n",
    "\n",
    "if randomsample == 'y':\n",
    "    warcs = random.sample(warcs, number_warcs_to_analyze)\n",
    "else:\n",
    "  warcs = warcs[-number_warcs_to_analyze:]\n",
    "\n",
    "for index, warc in enumerate(warcs):\n",
    "    warcs[index] = 'https://data.commoncrawl.org/' + warc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to load model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-04 16:25:52.213187: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded\n"
     ]
    }
   ],
   "source": [
    "path_dl_model = './models/model_dl'\n",
    "print('starting to load model')\n",
    "FINDMODEL = PipelineModel.load(path_dl_model)\n",
    "print('model loaded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def drop_nonfinance_articles(df):\n",
    "  df = FINDMODEL.transform(df)\n",
    "  df = df.withColumn('finance', df['financial_model_pred.result'].getItem(0).cast('float'))\n",
    "  df = df.filter(df['finance'] == 1.0)\n",
    "  return df\n",
    "\n",
    "\n",
    "#function to convert time from commoncrawl format to y-m-d\n",
    "def convert_header_date(date):\n",
    "    return time.strftime('%Y-%m-%d', time.strptime(date, '%Y-%m-%dT%H:%M:%SZ'))\n",
    "\n",
    "\n",
    "#obtaining stock data from yahoo finance from 2019 to current date.\n",
    "currentdate = time.strftime(\"%Y-%m-%d\")\n",
    "stockdata = yf.download(ticker, start='2010-01-01', end=currentdate)['Adj Close']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warc done\n",
      "warc done\n",
      "warc done\n",
      "warc done\n",
      "warc done\n",
      "warc done\n",
      "warc done\n",
      "warc done\n",
      "date not in stockdata 2023-05-04\n",
      "date not in stockdata 2023-05-04\n",
      "date not in stockdata 2023-05-04\n",
      "date not in stockdata 2023-05-04\n",
      "date not in stockdata 2023-05-04\n",
      "date not in stockdata 2023-05-04\n",
      "date not in stockdata 2023-05-04\n",
      "date not in stockdata 2023-05-04\n",
      "date not in stockdata 2023-05-04\n",
      "date not in stockdata 2023-05-04\n",
      "warc done\n",
      "date not in stockdata 2023-05-04\n",
      "date not in stockdata 2023-05-04\n",
      "date not in stockdata 2023-05-04\n",
      "date not in stockdata 2023-05-04\n",
      "date not in stockdata 2023-05-04\n",
      "date not in stockdata 2023-05-04\n",
      "date not in stockdata 2023-05-04\n",
      "date not in stockdata 2023-05-04\n",
      "date not in stockdata 2023-05-04\n",
      "date not in stockdata 2023-05-04\n",
      "date not in stockdata 2023-05-04\n",
      "size of data:  49\n",
      "done\n",
      "failures:  1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# creating the main rdd to store the data\n",
    "#creating scehma to store text and prices\n",
    "data = StructType([\\\n",
    "  StructField(\"text\", StringType(), True),\n",
    "    StructField(\"price\", StringType(), True),\n",
    "    StructField(\"date\", StringType(), True)  \n",
    "]\n",
    ")\n",
    "\n",
    "list_of_rows_batch = []\n",
    "rows_batch_len = 0\n",
    "recordsfetched = 0\n",
    "failures = 0\n",
    "total = 0 \n",
    "for warc_url in warcs:\n",
    "    response = requests.get(warc_url, stream=True)\n",
    "    if response.ok!=True:\n",
    "        raise Exception(\"Error downloading WARC file\")\n",
    "    records = ArchiveIterator(response.raw, arc2warc=True)\n",
    "    #what this should do is write each record's plaintexxt to a csv file\n",
    "    for record in records:\n",
    "        if recordsfetched >= num_records_percrawl:\n",
    "            total += recordsfetched\n",
    "            recordsfetched = 0\n",
    "            print(\"warc done\")\n",
    "            break\n",
    "        if record.rec_type == 'response':\n",
    "            try: \n",
    "                html = record.content_stream().read() .decode('utf-8')\n",
    "                plaintext = BeautifulSoup(html, 'lxml').get_text()\n",
    "                plaintext = re.sub(r'\\s+', ' ', plaintext)\n",
    "                plaintext = re.sub(r'[^a-zA-Z0-9\\s]', '', plaintext).lower()\n",
    "\n",
    "                #obtains plaintext from the html\n",
    "                if detect(plaintext) == 'en' and len(plaintext) > 150:  #TODO add classifier here later to ensure its a financial article\n",
    "                    date = record.rec_headers.get_header('WARC-Date')\n",
    "                    date = convert_header_date(date)\n",
    "                    # append the plaintext and price to the batch\n",
    "                    if date in stockdata.index:\n",
    "                        list_of_rows_batch.append({'text':plaintext, 'price':float(stockdata[date]), 'date':date})\n",
    "                        recordsfetched += 1\n",
    "                        rows_batch_len += 1\n",
    "                    else:\n",
    "                        print('date not in stockdata',date)\n",
    "                        #likely a weekend or holiday, so we will just skip the entire warc\n",
    "                        break\n",
    "                else:\n",
    "                    recordsfetched += 1                          \n",
    "            except:\n",
    "                recordsfetched += 1  # because if the entire warc file is not in english or wrong date, we still want to move on to the next one\n",
    "                failures += 1\n",
    "                #print(\"attempt record: \", record.rec_headers.get_header('WARC-Target-URI'), \" failed\")\n",
    "                pass\n",
    "\n",
    "    \n",
    "df = spark.createDataFrame(list_of_rows_batch, data)\n",
    "print(\"size of data: \", total)\n",
    "print(\"done\")\n",
    "print(\"failures: \", failures)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort out the finance articles\n",
    "df = drop_nonfinance_articles(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INFERENCE\n",
    "df = pipeline.fit(df).transform(df)\n",
    "\n",
    "df = df.withColumn(\"sentiment_score\", concat_ws(\",\", \"sentiment_score.result\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 38:=================================>                        (4 + 3) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|sentiment_score|\n",
      "+---------------+\n",
      "|positive       |\n",
      "|positive       |\n",
      "|positive       |\n",
      "|negative       |\n",
      "|positive       |\n",
      "+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#show the df sentiment_score column\n",
    "df.select('sentiment_score').show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLOTTING CODE\n",
    "\n",
    "from pyspark.sql.functions import col, avg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "agg_df = df.groupBy(\"date\").agg(avg(\"sentiment_score\").alias(\"average_sentiment\"))\n",
    "\n",
    "# Convert the aggregated PySpark DataFrame to a Pandas DataFrame\n",
    "pd_agg_df = agg_df.toPandas()\n",
    "\n",
    "# Convert 'date' column to pandas datetime object for better plotting\n",
    "pd_agg_df['date'] = pd.to_datetime(pd_agg_df['date'])\n",
    "\n",
    "# Sort the DataFrame by date\n",
    "pd_agg_df = pd_agg_df.sort_values(by=\"date\")\n",
    "\n",
    "# Plot the average sentiment score as a function of date\n",
    "plt.plot(pd_agg_df[\"date\"], pd_agg_df[\"average_sentiment\"])\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Average Sentiment Score\")\n",
    "plt.title(\"Average Sentiment Score by Date\")\n",
    "plt.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "am111_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f57737e6d1a902c43fc81f11403a4faf60173bd712443272aa63091abb3c628b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
